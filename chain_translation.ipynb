{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chain translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSqkl9XKH6M_",
        "outputId": "bb869ca9-5980-4f25-d052-08fc4cb9a2a5"
      },
      "source": [
        "pip install opencc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencc in /usr/local/lib/python3.7/dist-packages (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1-SfLcIBOP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from opencc import OpenCC\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "cc = OpenCC('t2s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JESFEk0lIPPM"
      },
      "source": [
        "#Define an Encoder \n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTQd2YNOITBe"
      },
      "source": [
        "#Define a BahdanauAttention\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # The shape of the hidden layer == (batch size, hidden layer size)\n",
        "    # The shape of hidden_with_time_axis  == （batch size，1, hidden layer size）\n",
        "    # calculate the score \n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # The shape of the score == (batch size, maximum length, 1)\n",
        "    # We get 1 on the last axis because we apply the score to self.V\n",
        "    # Before applying self.V, the shape of the tensor is (batch size, maximum length, unit)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    # The shape of attention_weights == (batch size, maximum length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    # The shape of context_vector == batch size, hidden layer size\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0iyIXjaIVnF"
      },
      "source": [
        "#Define a Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    # using attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # Word embedding encoding on X\n",
        "    x = self.embedding(x)\n",
        "    # The shape of x after concatenation == （batch size，1，embedding dimensions + hidden layer size）\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    #Send the merged vector to GRU\n",
        "    output, state = self.gru(x)\n",
        "    # The shape of output == （batch size * 1，hidden layer size）\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsQTq8WtIaBZ"
      },
      "source": [
        "d_model = 128\n",
        "# d_model = train_step_en_de(inp, targ, enc_hidden)\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=6000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDuUzNFmIgSA"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(0.003, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60NdpgRAIpEV",
        "outputId": "84001e8b-1e8c-4075-9a86-39b40de3c3ac"
      },
      "source": [
        "path_to_file = \"drive/MyDrive/Colab Notebooks/cmn-clean.txt\"\n",
        "path_to_file_de = \"drive/MyDrive/Colab Notebooks/deu.txt\"\n",
        "path_to_singleCorpus = \"drive/MyDrive/Colab Notebooks/single_corpus_zh.txt\"\n",
        "path_to_newCorpus = \"drive/MyDrive/Colab Notebooks/new_corpus.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "input_file = open(path_to_file,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mpOZjs9Iwg4"
      },
      "source": [
        "cc = OpenCC('t2s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6gf49X6Iykx"
      },
      "source": [
        "#Data cleaning\n",
        "def preprocess_sentence(w): \n",
        "    w = re.sub(r\"([\\u4e00-\\u9fa5_?.!,:：？。，！])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    # Replace all characters with spaces except\n",
        "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^\\u4e00-\\u9fa5\\u0080-\\uFFFF_a-zA-Z0-9?.!,:：？。，！']+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    w = '<s> ' + w + ' <s/>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jndDPE3gI1t-"
      },
      "source": [
        "def create_dataset(path):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines]\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BrB95O-I4Aa"
      },
      "source": [
        "# Calculate the maximum length of Tensor\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPz-k5VI8oI"
      },
      "source": [
        "# Tokenize\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  print (tensor),print(lang_tokenizer)\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7MQfY9TI-Ky"
      },
      "source": [
        "# Set the length of the input sequence\n",
        "def uniformLength(list1,list2,maxlen,minlen):\n",
        "  print(len(list1),len(list2))\n",
        "  length = len(list1)\n",
        "  a= 0\n",
        "  l =0\n",
        "  while length != l:\n",
        "    l = len(list1)\n",
        "    for i in range(a,length):\n",
        "      if (len(list1[i].strip().split())> maxlen or len(list1[i].strip().split())< minlen\n",
        "          or len(list2[i].strip().split())> maxlen or len(list2[i].strip().split())< minlen):\n",
        "        del list1[i]\n",
        "        del list2[i]\n",
        "        length = len(list2)\n",
        "        a=i\n",
        "        break\n",
        "  return list1,list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0gwKNNMJAhw"
      },
      "source": [
        "# Load the dataset\n",
        "def load_dataset(path,maxlen,minlen):\n",
        "    # Create cleaned input and output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    targ_lang=list(targ_lang)\n",
        "    inp_lang=list(inp_lang)\n",
        "    targ_lang, inp_lang =uniformLength(targ_lang, inp_lang,maxlen,minlen)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzgpsDkvJDrU",
        "outputId": "82a0dfef-4319-4bd6-99dc-5112c0c8a9d8"
      },
      "source": [
        "de_tensor, en_de_tensor, de_lang, en_de_lang = load_dataset(path_to_file_de,15,5)\n",
        "# Calculate the maximum length of the target tensor （max_length）\n",
        "max_length_targ, max_length_inp = max_length(de_tensor), max_length(en_de_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240828 240828\n",
            "[[    1   397    14 ...     0     0     0]\n",
            " [    1   748    14 ...     0     0     0]\n",
            " [    1   394   457 ...     0     0     0]\n",
            " ...\n",
            " [    1    19 34034 ...     2     0     0]\n",
            " [    1    32   254 ...  6169     3     2]\n",
            " [    1    14     8 ... 34041     3     2]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fbea2bcfb10>\n",
            "[[   1   13   17 ...    0    0    0]\n",
            " [   1   13   17 ...    0    0    0]\n",
            " [   1   49   35 ...    0    0    0]\n",
            " ...\n",
            " [   1   77 6544 ... 3609    3    2]\n",
            " [   1 4203 4144 ...    3    2    0]\n",
            " [   1  192   10 ...  293    3    2]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fbea1123950>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EC0L8nSJOcu",
        "outputId": "8c0423b6-c664-4f7c-e05a-ed35f930a541"
      },
      "source": [
        "# Split training set and validation set\n",
        "en_de_tensor_train, en_de_tensor_val, de_tensor_train, de_tensor_val = train_test_split(en_de_tensor, de_tensor, test_size=0.7)\n",
        "\n",
        "# 显示长度\n",
        "print(len(en_de_tensor_train), len(de_tensor_train), len(en_de_tensor_val), len(de_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68523 68523 159890 159890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzVQAVO0JRY5"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGvNiyJpJWAU",
        "outputId": "8420de5f-b4df-490e-bbb6-e4a83d728513"
      },
      "source": [
        "print (\"Input test Language; index to word mapping\")\n",
        "convert(en_de_lang, en_de_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target train Language; index to word mapping\")\n",
        "convert(de_lang, de_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input test Language; index to word mapping\n",
            "1 ----> <s>\n",
            "6 ----> i\n",
            "96 ----> got\n",
            "197 ----> these\n",
            "2894 ----> earrings\n",
            "78 ----> from\n",
            "25 ----> my\n",
            "1341 ----> grandmother\n",
            "3 ----> .\n",
            "2 ----> <s/>\n",
            "\n",
            "Target train Language; index to word mapping\n",
            "1 ----> <s>\n",
            "5 ----> ich\n",
            "22 ----> habe\n",
            "92 ----> diese\n",
            "3601 ----> ohrringe\n",
            "50 ----> von\n",
            "252 ----> meiner\n",
            "1394 ----> großmutter\n",
            "314 ----> bekommen\n",
            "3 ----> .\n",
            "2 ----> <s/>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYqlvhozJyS-"
      },
      "source": [
        "#Set model parameters\n",
        "BUFFER_SIZE = len(en_de_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(en_de_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_en_de_size = len(en_de_lang.word_index)+1\n",
        "vocab_de_size = len(de_lang.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWx90j-oJ3Kp"
      },
      "source": [
        "dataset_en_de = tf.data.Dataset.from_tensor_slices((en_de_tensor_train, de_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset_en_de = dataset_en_de.batch((BATCH_SIZE), drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZWojucWKKJD"
      },
      "source": [
        "encoder = Encoder(vocab_en_de_size, embedding_dim, units, BATCH_SIZE)\n",
        "attention_layer = BahdanauAttention(10)\n",
        "decoder = Decoder(vocab_de_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qqrYHcMKIj9"
      },
      "source": [
        "#Save the model training results using checkpoints\n",
        "checkpoint_dir = 'drive/MyDrive/Colab Notebooks/training_checkpoints_en_de'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-mJcovHKVEJ"
      },
      "source": [
        "# Model training\n",
        "@tf.function\n",
        "def train_step_en_de(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([de_lang.word_index['<s>']] * BATCH_SIZE, 1)\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      #Using teaching enforcing - Use the target word as the next input\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-5wPqJBKZ5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bfeed1-7453-4b6e-d6f8-db358861e744"
      },
      "source": [
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset_en_de.take(steps_per_epoch)):\n",
        "    batch_loss = train_step_en_de(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # store the checkpoint\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.6415\n",
            "Epoch 1 Batch 100 Loss 2.7966\n",
            "Epoch 1 Batch 200 Loss 2.5998\n",
            "Epoch 1 Batch 300 Loss 2.3064\n",
            "Epoch 1 Batch 400 Loss 2.0900\n",
            "Epoch 1 Batch 500 Loss 2.4119\n",
            "Epoch 1 Batch 600 Loss 2.1364\n",
            "Epoch 1 Batch 700 Loss 2.2787\n",
            "Epoch 1 Batch 800 Loss 2.1781\n",
            "Epoch 1 Batch 900 Loss 2.2127\n",
            "Epoch 1 Batch 1000 Loss 2.0359\n",
            "Epoch 1 Loss 2.3381\n",
            "Time taken for 1 epoch 199.91803431510925 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.0389\n",
            "Epoch 2 Batch 100 Loss 1.9963\n",
            "Epoch 2 Batch 200 Loss 2.0377\n",
            "Epoch 2 Batch 300 Loss 2.0651\n",
            "Epoch 2 Batch 400 Loss 1.9092\n",
            "Epoch 2 Batch 500 Loss 2.1306\n",
            "Epoch 2 Batch 600 Loss 2.1743\n",
            "Epoch 2 Batch 700 Loss 1.7322\n",
            "Epoch 2 Batch 800 Loss 1.8333\n",
            "Epoch 2 Batch 900 Loss 1.7873\n",
            "Epoch 2 Batch 1000 Loss 1.8747\n",
            "Epoch 2 Loss 1.9012\n",
            "Time taken for 1 epoch 178.14481568336487 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3213\n",
            "Epoch 3 Batch 100 Loss 1.6131\n",
            "Epoch 3 Batch 200 Loss 1.4259\n",
            "Epoch 3 Batch 300 Loss 1.4720\n",
            "Epoch 3 Batch 400 Loss 1.4512\n",
            "Epoch 3 Batch 500 Loss 1.3531\n",
            "Epoch 3 Batch 600 Loss 1.3211\n",
            "Epoch 3 Batch 700 Loss 1.3135\n",
            "Epoch 3 Batch 800 Loss 1.5926\n",
            "Epoch 3 Batch 900 Loss 1.3613\n",
            "Epoch 3 Batch 1000 Loss 1.3857\n",
            "Epoch 3 Loss 1.4149\n",
            "Time taken for 1 epoch 176.85275506973267 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0154\n",
            "Epoch 4 Batch 100 Loss 1.1364\n",
            "Epoch 4 Batch 200 Loss 0.8816\n",
            "Epoch 4 Batch 300 Loss 1.1411\n",
            "Epoch 4 Batch 400 Loss 1.0238\n",
            "Epoch 4 Batch 500 Loss 1.0704\n",
            "Epoch 4 Batch 600 Loss 1.2256\n",
            "Epoch 4 Batch 700 Loss 0.8241\n",
            "Epoch 4 Batch 800 Loss 1.2226\n",
            "Epoch 4 Batch 900 Loss 1.1082\n",
            "Epoch 4 Batch 1000 Loss 1.2845\n",
            "Epoch 4 Loss 1.0967\n",
            "Time taken for 1 epoch 180.43072199821472 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7662\n",
            "Epoch 5 Batch 100 Loss 0.7440\n",
            "Epoch 5 Batch 200 Loss 0.8483\n",
            "Epoch 5 Batch 300 Loss 1.0702\n",
            "Epoch 5 Batch 400 Loss 0.9305\n",
            "Epoch 5 Batch 500 Loss 0.7368\n",
            "Epoch 5 Batch 600 Loss 0.8737\n",
            "Epoch 5 Batch 700 Loss 1.0112\n",
            "Epoch 5 Batch 800 Loss 0.8920\n",
            "Epoch 5 Batch 900 Loss 1.0872\n",
            "Epoch 5 Batch 1000 Loss 0.8985\n",
            "Epoch 5 Loss 0.8876\n",
            "Time taken for 1 epoch 177.30156016349792 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6513\n",
            "Epoch 6 Batch 100 Loss 0.6132\n",
            "Epoch 6 Batch 200 Loss 0.7900\n",
            "Epoch 6 Batch 300 Loss 0.7559\n",
            "Epoch 6 Batch 400 Loss 0.6576\n",
            "Epoch 6 Batch 500 Loss 0.8178\n",
            "Epoch 6 Batch 600 Loss 0.8546\n",
            "Epoch 6 Batch 700 Loss 0.7262\n",
            "Epoch 6 Batch 800 Loss 0.7285\n",
            "Epoch 6 Batch 900 Loss 0.8620\n",
            "Epoch 6 Batch 1000 Loss 0.7643\n",
            "Epoch 6 Loss 0.7218\n",
            "Time taken for 1 epoch 179.19550108909607 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4890\n",
            "Epoch 7 Batch 100 Loss 0.5026\n",
            "Epoch 7 Batch 200 Loss 0.4818\n",
            "Epoch 7 Batch 300 Loss 0.4815\n",
            "Epoch 7 Batch 400 Loss 0.5957\n",
            "Epoch 7 Batch 500 Loss 0.5649\n",
            "Epoch 7 Batch 600 Loss 0.6384\n",
            "Epoch 7 Batch 700 Loss 0.6225\n",
            "Epoch 7 Batch 800 Loss 0.6813\n",
            "Epoch 7 Batch 900 Loss 0.7437\n",
            "Epoch 7 Batch 1000 Loss 0.8192\n",
            "Epoch 7 Loss 0.5888\n",
            "Time taken for 1 epoch 176.0978729724884 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5400\n",
            "Epoch 8 Batch 100 Loss 0.3819\n",
            "Epoch 8 Batch 200 Loss 0.3851\n",
            "Epoch 8 Batch 300 Loss 0.3624\n",
            "Epoch 8 Batch 400 Loss 0.4715\n",
            "Epoch 8 Batch 500 Loss 0.4659\n",
            "Epoch 8 Batch 600 Loss 0.5600\n",
            "Epoch 8 Batch 700 Loss 0.5797\n",
            "Epoch 8 Batch 800 Loss 0.4698\n",
            "Epoch 8 Batch 900 Loss 0.5116\n",
            "Epoch 8 Batch 1000 Loss 0.6926\n",
            "Epoch 8 Loss 0.4954\n",
            "Time taken for 1 epoch 179.01375889778137 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3138\n",
            "Epoch 9 Batch 100 Loss 0.3403\n",
            "Epoch 9 Batch 200 Loss 0.3757\n",
            "Epoch 9 Batch 300 Loss 0.3909\n",
            "Epoch 9 Batch 400 Loss 0.4026\n",
            "Epoch 9 Batch 500 Loss 0.4438\n",
            "Epoch 9 Batch 600 Loss 0.4680\n",
            "Epoch 9 Batch 700 Loss 0.4465\n",
            "Epoch 9 Batch 800 Loss 0.5711\n",
            "Epoch 9 Batch 900 Loss 0.4154\n",
            "Epoch 9 Batch 1000 Loss 0.5882\n",
            "Epoch 9 Loss 0.4287\n",
            "Time taken for 1 epoch 178.09108448028564 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3015\n",
            "Epoch 10 Batch 100 Loss 0.2798\n",
            "Epoch 10 Batch 200 Loss 0.3840\n",
            "Epoch 10 Batch 300 Loss 0.3235\n",
            "Epoch 10 Batch 400 Loss 0.3284\n",
            "Epoch 10 Batch 500 Loss 0.3497\n",
            "Epoch 10 Batch 600 Loss 0.4017\n",
            "Epoch 10 Batch 700 Loss 0.4247\n",
            "Epoch 10 Batch 800 Loss 0.4178\n",
            "Epoch 10 Batch 900 Loss 0.5132\n",
            "Epoch 10 Batch 1000 Loss 0.4485\n",
            "Epoch 10 Loss 0.3806\n",
            "Time taken for 1 epoch 179.7487654685974 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2492\n",
            "Epoch 11 Batch 100 Loss 0.2177\n",
            "Epoch 11 Batch 200 Loss 0.2852\n",
            "Epoch 11 Batch 300 Loss 0.3277\n",
            "Epoch 11 Batch 400 Loss 0.3314\n",
            "Epoch 11 Batch 500 Loss 0.3531\n",
            "Epoch 11 Batch 600 Loss 0.3403\n",
            "Epoch 11 Batch 700 Loss 0.2824\n",
            "Epoch 11 Batch 800 Loss 0.3674\n",
            "Epoch 11 Batch 900 Loss 0.4159\n",
            "Epoch 11 Batch 1000 Loss 0.4873\n",
            "Epoch 11 Loss 0.3418\n",
            "Time taken for 1 epoch 177.5093560218811 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2085\n",
            "Epoch 12 Batch 100 Loss 0.2460\n",
            "Epoch 12 Batch 200 Loss 0.2734\n",
            "Epoch 12 Batch 300 Loss 0.2299\n",
            "Epoch 12 Batch 400 Loss 0.3118\n",
            "Epoch 12 Batch 500 Loss 0.3126\n",
            "Epoch 12 Batch 600 Loss 0.3590\n",
            "Epoch 12 Batch 700 Loss 0.3327\n",
            "Epoch 12 Batch 800 Loss 0.3228\n",
            "Epoch 12 Batch 900 Loss 0.3796\n",
            "Epoch 12 Batch 1000 Loss 0.3589\n",
            "Epoch 12 Loss 0.3106\n",
            "Time taken for 1 epoch 178.76332116127014 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1891\n",
            "Epoch 13 Batch 100 Loss 0.2028\n",
            "Epoch 13 Batch 200 Loss 0.1953\n",
            "Epoch 13 Batch 300 Loss 0.2870\n",
            "Epoch 13 Batch 400 Loss 0.2481\n",
            "Epoch 13 Batch 500 Loss 0.2068\n",
            "Epoch 13 Batch 600 Loss 0.3269\n",
            "Epoch 13 Batch 700 Loss 0.2925\n",
            "Epoch 13 Batch 800 Loss 0.4107\n",
            "Epoch 13 Batch 900 Loss 0.2835\n",
            "Epoch 13 Batch 1000 Loss 0.3233\n",
            "Epoch 13 Loss 0.2873\n",
            "Time taken for 1 epoch 176.97087240219116 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2123\n",
            "Epoch 14 Batch 100 Loss 0.2054\n",
            "Epoch 14 Batch 200 Loss 0.2125\n",
            "Epoch 14 Batch 300 Loss 0.2172\n",
            "Epoch 14 Batch 400 Loss 0.2471\n",
            "Epoch 14 Batch 500 Loss 0.2847\n",
            "Epoch 14 Batch 600 Loss 0.2957\n",
            "Epoch 14 Batch 700 Loss 0.2545\n",
            "Epoch 14 Batch 800 Loss 0.3885\n",
            "Epoch 14 Batch 900 Loss 0.2961\n",
            "Epoch 14 Batch 1000 Loss 0.3794\n",
            "Epoch 14 Loss 0.2708\n",
            "Time taken for 1 epoch 179.6495759487152 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1886\n",
            "Epoch 15 Batch 100 Loss 0.1770\n",
            "Epoch 15 Batch 200 Loss 0.2092\n",
            "Epoch 15 Batch 300 Loss 0.2380\n",
            "Epoch 15 Batch 400 Loss 0.1988\n",
            "Epoch 15 Batch 500 Loss 0.2620\n",
            "Epoch 15 Batch 600 Loss 0.2670\n",
            "Epoch 15 Batch 700 Loss 0.2686\n",
            "Epoch 15 Batch 800 Loss 0.2906\n",
            "Epoch 15 Batch 900 Loss 0.2580\n",
            "Epoch 15 Batch 1000 Loss 0.2552\n",
            "Epoch 15 Loss 0.2564\n",
            "Time taken for 1 epoch 178.18669652938843 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1759\n",
            "Epoch 16 Batch 100 Loss 0.1827\n",
            "Epoch 16 Batch 200 Loss 0.2157\n",
            "Epoch 16 Batch 300 Loss 0.2221\n",
            "Epoch 16 Batch 400 Loss 0.2663\n",
            "Epoch 16 Batch 500 Loss 0.2352\n",
            "Epoch 16 Batch 600 Loss 0.2827\n",
            "Epoch 16 Batch 700 Loss 0.2101\n",
            "Epoch 16 Batch 800 Loss 0.2237\n",
            "Epoch 16 Batch 900 Loss 0.3143\n",
            "Epoch 16 Batch 1000 Loss 0.3587\n",
            "Epoch 16 Loss 0.2450\n",
            "Time taken for 1 epoch 180.2388973236084 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1532\n",
            "Epoch 17 Batch 100 Loss 0.1440\n",
            "Epoch 17 Batch 200 Loss 0.2605\n",
            "Epoch 17 Batch 300 Loss 0.2255\n",
            "Epoch 17 Batch 400 Loss 0.2570\n",
            "Epoch 17 Batch 500 Loss 0.1584\n",
            "Epoch 17 Batch 600 Loss 0.2569\n",
            "Epoch 17 Batch 700 Loss 0.3078\n",
            "Epoch 17 Batch 800 Loss 0.2954\n",
            "Epoch 17 Batch 900 Loss 0.2392\n",
            "Epoch 17 Batch 1000 Loss 0.2813\n",
            "Epoch 17 Loss 0.2361\n",
            "Time taken for 1 epoch 179.20095539093018 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1449\n",
            "Epoch 18 Batch 100 Loss 0.1593\n",
            "Epoch 18 Batch 200 Loss 0.2166\n",
            "Epoch 18 Batch 300 Loss 0.1669\n",
            "Epoch 18 Batch 400 Loss 0.2243\n",
            "Epoch 18 Batch 500 Loss 0.2285\n",
            "Epoch 18 Batch 600 Loss 0.2193\n",
            "Epoch 18 Batch 700 Loss 0.2466\n",
            "Epoch 18 Batch 800 Loss 0.2839\n",
            "Epoch 18 Batch 900 Loss 0.2241\n",
            "Epoch 18 Batch 1000 Loss 0.2422\n",
            "Epoch 18 Loss 0.2284\n",
            "Time taken for 1 epoch 182.01405310630798 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1942\n",
            "Epoch 19 Batch 100 Loss 0.1617\n",
            "Epoch 19 Batch 200 Loss 0.1721\n",
            "Epoch 19 Batch 300 Loss 0.2323\n",
            "Epoch 19 Batch 400 Loss 0.1861\n",
            "Epoch 19 Batch 500 Loss 0.2675\n",
            "Epoch 19 Batch 600 Loss 0.2323\n",
            "Epoch 19 Batch 700 Loss 0.2868\n",
            "Epoch 19 Batch 800 Loss 0.2558\n",
            "Epoch 19 Batch 900 Loss 0.2992\n",
            "Epoch 19 Batch 1000 Loss 0.3353\n",
            "Epoch 19 Loss 0.2223\n",
            "Time taken for 1 epoch 178.31998300552368 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1489\n",
            "Epoch 20 Batch 100 Loss 0.2139\n",
            "Epoch 20 Batch 200 Loss 0.1881\n",
            "Epoch 20 Batch 300 Loss 0.2503\n",
            "Epoch 20 Batch 400 Loss 0.2132\n",
            "Epoch 20 Batch 500 Loss 0.2249\n",
            "Epoch 20 Batch 600 Loss 0.2215\n",
            "Epoch 20 Batch 700 Loss 0.2508\n",
            "Epoch 20 Batch 800 Loss 0.2752\n",
            "Epoch 20 Batch 900 Loss 0.2385\n",
            "Epoch 20 Batch 1000 Loss 0.2565\n",
            "Epoch 20 Loss 0.2171\n",
            "Time taken for 1 epoch 180.5922167301178 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1508\n",
            "Epoch 21 Batch 100 Loss 0.1687\n",
            "Epoch 21 Batch 200 Loss 0.2309\n",
            "Epoch 21 Batch 300 Loss 0.2165\n",
            "Epoch 21 Batch 400 Loss 0.2114\n",
            "Epoch 21 Batch 500 Loss 0.2167\n",
            "Epoch 21 Batch 600 Loss 0.2043\n",
            "Epoch 21 Batch 700 Loss 0.1902\n",
            "Epoch 21 Batch 800 Loss 0.1967\n",
            "Epoch 21 Batch 900 Loss 0.2184\n",
            "Epoch 21 Batch 1000 Loss 0.2925\n",
            "Epoch 21 Loss 0.2102\n",
            "Time taken for 1 epoch 178.2463676929474 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1418\n",
            "Epoch 22 Batch 100 Loss 0.1546\n",
            "Epoch 22 Batch 200 Loss 0.1733\n",
            "Epoch 22 Batch 300 Loss 0.1852\n",
            "Epoch 22 Batch 400 Loss 0.1839\n",
            "Epoch 22 Batch 500 Loss 0.2562\n",
            "Epoch 22 Batch 600 Loss 0.2422\n",
            "Epoch 22 Batch 700 Loss 0.2806\n",
            "Epoch 22 Batch 800 Loss 0.2174\n",
            "Epoch 22 Batch 900 Loss 0.2469\n",
            "Epoch 22 Batch 1000 Loss 0.2361\n",
            "Epoch 22 Loss 0.2051\n",
            "Time taken for 1 epoch 181.8414900302887 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1751\n",
            "Epoch 23 Batch 100 Loss 0.1116\n",
            "Epoch 23 Batch 200 Loss 0.1291\n",
            "Epoch 23 Batch 300 Loss 0.1631\n",
            "Epoch 23 Batch 400 Loss 0.2166\n",
            "Epoch 23 Batch 500 Loss 0.2308\n",
            "Epoch 23 Batch 600 Loss 0.2119\n",
            "Epoch 23 Batch 700 Loss 0.1916\n",
            "Epoch 23 Batch 800 Loss 0.2389\n",
            "Epoch 23 Batch 900 Loss 0.1913\n",
            "Epoch 23 Batch 1000 Loss 0.2611\n",
            "Epoch 23 Loss 0.2003\n",
            "Time taken for 1 epoch 178.29726219177246 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1476\n",
            "Epoch 24 Batch 100 Loss 0.1468\n",
            "Epoch 24 Batch 200 Loss 0.1624\n",
            "Epoch 24 Batch 300 Loss 0.1461\n",
            "Epoch 24 Batch 400 Loss 0.1551\n",
            "Epoch 24 Batch 500 Loss 0.2505\n",
            "Epoch 24 Batch 600 Loss 0.2155\n",
            "Epoch 24 Batch 700 Loss 0.1881\n",
            "Epoch 24 Batch 800 Loss 0.2208\n",
            "Epoch 24 Batch 900 Loss 0.2290\n",
            "Epoch 24 Batch 1000 Loss 0.2823\n",
            "Epoch 24 Loss 0.2011\n",
            "Time taken for 1 epoch 181.20250582695007 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1374\n",
            "Epoch 25 Batch 100 Loss 0.1095\n",
            "Epoch 25 Batch 200 Loss 0.1605\n",
            "Epoch 25 Batch 300 Loss 0.1969\n",
            "Epoch 25 Batch 400 Loss 0.1863\n",
            "Epoch 25 Batch 500 Loss 0.1635\n",
            "Epoch 25 Batch 600 Loss 0.1490\n",
            "Epoch 25 Batch 700 Loss 0.1994\n",
            "Epoch 25 Batch 800 Loss 0.2498\n",
            "Epoch 25 Batch 900 Loss 0.2074\n",
            "Epoch 25 Batch 1000 Loss 0.2240\n",
            "Epoch 25 Loss 0.1979\n",
            "Time taken for 1 epoch 178.2706618309021 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.1819\n",
            "Epoch 26 Batch 100 Loss 0.1368\n",
            "Epoch 26 Batch 200 Loss 0.1790\n",
            "Epoch 26 Batch 300 Loss 0.1574\n",
            "Epoch 26 Batch 400 Loss 0.1734\n",
            "Epoch 26 Batch 500 Loss 0.2021\n",
            "Epoch 26 Batch 600 Loss 0.2346\n",
            "Epoch 26 Batch 700 Loss 0.2306\n",
            "Epoch 26 Batch 800 Loss 0.1854\n",
            "Epoch 26 Batch 900 Loss 0.2367\n",
            "Epoch 26 Batch 1000 Loss 0.2367\n",
            "Epoch 26 Loss 0.1973\n",
            "Time taken for 1 epoch 181.94863057136536 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0883\n",
            "Epoch 27 Batch 100 Loss 0.1478\n",
            "Epoch 27 Batch 200 Loss 0.1687\n",
            "Epoch 27 Batch 300 Loss 0.2165\n",
            "Epoch 27 Batch 400 Loss 0.1862\n",
            "Epoch 27 Batch 500 Loss 0.1688\n",
            "Epoch 27 Batch 600 Loss 0.2185\n",
            "Epoch 27 Batch 700 Loss 0.1953\n",
            "Epoch 27 Batch 800 Loss 0.1935\n",
            "Epoch 27 Batch 900 Loss 0.2377\n",
            "Epoch 27 Batch 1000 Loss 0.2468\n",
            "Epoch 27 Loss 0.1976\n",
            "Time taken for 1 epoch 178.93400120735168 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1252\n",
            "Epoch 28 Batch 100 Loss 0.1470\n",
            "Epoch 28 Batch 200 Loss 0.1277\n",
            "Epoch 28 Batch 300 Loss 0.1389\n",
            "Epoch 28 Batch 400 Loss 0.2376\n",
            "Epoch 28 Batch 500 Loss 0.2522\n",
            "Epoch 28 Batch 600 Loss 0.1903\n",
            "Epoch 28 Batch 700 Loss 0.1560\n",
            "Epoch 28 Batch 800 Loss 0.1880\n",
            "Epoch 28 Batch 900 Loss 0.2017\n",
            "Epoch 28 Batch 1000 Loss 0.1925\n",
            "Epoch 28 Loss 0.1969\n",
            "Time taken for 1 epoch 180.9053463935852 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0936\n",
            "Epoch 29 Batch 100 Loss 0.1810\n",
            "Epoch 29 Batch 200 Loss 0.1664\n",
            "Epoch 29 Batch 300 Loss 0.1776\n",
            "Epoch 29 Batch 400 Loss 0.2020\n",
            "Epoch 29 Batch 500 Loss 0.1936\n",
            "Epoch 29 Batch 600 Loss 0.2025\n",
            "Epoch 29 Batch 700 Loss 0.2574\n",
            "Epoch 29 Batch 800 Loss 0.2388\n",
            "Epoch 29 Batch 900 Loss 0.1846\n",
            "Epoch 29 Batch 1000 Loss 0.2168\n",
            "Epoch 29 Loss 0.1955\n",
            "Time taken for 1 epoch 177.27323627471924 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.1045\n",
            "Epoch 30 Batch 100 Loss 0.1219\n",
            "Epoch 30 Batch 200 Loss 0.1481\n",
            "Epoch 30 Batch 300 Loss 0.2010\n",
            "Epoch 30 Batch 400 Loss 0.2385\n",
            "Epoch 30 Batch 500 Loss 0.2052\n",
            "Epoch 30 Batch 600 Loss 0.1655\n",
            "Epoch 30 Batch 700 Loss 0.1992\n",
            "Epoch 30 Batch 800 Loss 0.1961\n",
            "Epoch 30 Batch 900 Loss 0.1929\n",
            "Epoch 30 Batch 1000 Loss 0.2586\n",
            "Epoch 30 Loss 0.1976\n",
            "Time taken for 1 epoch 180.51606512069702 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu9BFTE-fDj-"
      },
      "source": [
        "# Evaluate the trained model\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    strinfo = re.compile('<s/>')\n",
        "    sentence = strinfo.sub('', sentence).strip()\n",
        "    strinfo = re.compile('<s>')\n",
        "    sentence = strinfo.sub('', sentence).strip()\n",
        "    string = ''\n",
        "    for i in sentence.split(' '):\n",
        "      try:\n",
        "        en_de_lang.word_index[i]\n",
        "        string=string+i+' '\n",
        "      except Exception:\n",
        "        string=string+' un '\n",
        "    sentence = string \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [en_de_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([de_lang.word_index['<s>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += de_lang.index_word[predicted_id] + ' '\n",
        "        if de_lang.index_word[predicted_id] == '<s/>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHblME6XgtcE"
      },
      "source": [
        "#Translate the sentence\n",
        "def translate(sentence):\n",
        "    result, sentence,_ = evaluate(sentence)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "XAkkKg2VTN4J",
        "outputId": "ad4a88fd-b930-4bfb-8c26-69d3920f3b05"
      },
      "source": [
        "translate(u'i want to have a meeting with you.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[  1   6  38   7  20  10 378  37   5   3   2   0   0   0   0]], shape=(1, 15), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich möchte mit dir ein gespräch mit dir . <s/> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJmUZSsUnGAW"
      },
      "source": [
        "def merge(lang,tensor):\n",
        "  s=''\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      s=s+' '+lang.index_word[t]\n",
        "  strinfo = re.compile('<s>')\n",
        "  s = strinfo.sub('', s)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  s = strinfo.sub('', s)\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0aewd8_mo_v",
        "outputId": "9f9e6cca-3231-4b5d-d7d0-91a4af4fccb0"
      },
      "source": [
        "#BLEU score of English German Translation\n",
        "score = 0\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "for i in range(110):\n",
        "\n",
        "  sentence = cleanSentence(mono_en[i])\n",
        "  # sentence = merge(en_de_lang,sentence).strip()\n",
        "  # print(sentence)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  sentence_de = strinfo.sub('', sentence)\n",
        "  sentence_de = translate(sentence)\n",
        "  result = strinfo.sub('', sentence_de)\n",
        "  # print(result)\n",
        "  result = result.split()\n",
        "  reference = [cleanSentence(mono_de[i]).strip().split()]\n",
        "\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/110)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5374300329920739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20V0x1MgLX5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a03367-7ae5-4fb2-9cd0-63afdcae1302"
      },
      "source": [
        "path_mono_de = \"drive/MyDrive/Colab Notebooks/single_corpus_de.txt\"\n",
        "path_mono_en = \"drive/MyDrive/Colab Notebooks/single_corpus_en.txt\"\n",
        "path_mono_zh = \"drive/MyDrive/Colab Notebooks/single_corpus_zh.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "file_mono_de = open(path_mono_de,\"r\",encoding=\"utf-8\")\n",
        "file_mono_en = open(path_mono_en,\"r\",encoding=\"utf-8\")\n",
        "file_mono_zh = open(path_mono_zh,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOoMrf-z3MmU"
      },
      "source": [
        "def openFile(path):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # for line in lines:\n",
        "  #   line = preprocess_sentence(line)\n",
        "\n",
        "    # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines]\n",
        "    # print(word_pairs[200])\n",
        "\n",
        "  return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMIZvr_j5pR4"
      },
      "source": [
        "mono_de = openFile(path_mono_de)\n",
        "mono_en = openFile(path_mono_en)\n",
        "mono_zh = openFile(path_mono_zh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HUHVav3rL_u"
      },
      "source": [
        "def cleanSentence(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  strinfo1 = re.compile('<s>')\n",
        "  sentence = strinfo1.sub('', sentence).strip()\n",
        "  strinfo2 = re.compile('<s/>')\n",
        "  sentence = strinfo2.sub('', sentence).strip()\n",
        "  return sentence "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CJCb4gcfrDBY",
        "outputId": "2d6fb4a5-54a4-41fa-bd04-5f352efcf38e"
      },
      "source": [
        "cleanSentence(mono_de[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'durch weitere experimente fanden die wissenschaftler das heraus .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGcoz4l_wdKf"
      },
      "source": [
        "# Translate chinese to english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmVhywURwVuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b562685-f6ad-4bf0-b4b8-f0ad704f626b"
      },
      "source": [
        "# input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\n",
        "cn_tensor, en_cn_tensor, cn_lang, en_cn_lang = load_dataset(path_to_file,15,1)\n",
        "# target_tensor, input_tensor,  inp_lang, targ_lang = load_dataset(path_to_file)\n",
        "max_length_targ, max_length_inp = max_length(cn_tensor), max_length(en_cn_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26828 26828\n",
            "[[   1 2755    3 ...    0    0    0]\n",
            " [   1 1987    3 ...    0    0    0]\n",
            " [   1    6   31 ...    0    0    0]\n",
            " ...\n",
            " [   1  517   13 ...    0    0    0]\n",
            " [   1  732  444 ... 1581    3    2]\n",
            " [   1  517  602 ...    2    0    0]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fbe3463a350>\n",
            "[[   1 3814    3 ...    0    0    0]\n",
            " [   1 1289    3 ...    0    0    0]\n",
            " [   1 1289    3 ...    0    0    0]\n",
            " ...\n",
            " [   1    4  293 ...    0    0    0]\n",
            " [   1    4 1777 ...    0    0    0]\n",
            " [   1    4 1162 ...    0    0    0]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fbe34619710>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBqn-BRkwoUQ"
      },
      "source": [
        "cn_tensor_train, cn_tensor_val, en_cn_tensor_train, en_cn_tensor_val = train_test_split(cn_tensor, en_cn_tensor, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6HtGjw-w5V7"
      },
      "source": [
        "BUFFER_SIZE_CN = len(cn_tensor_train)\n",
        "steps_per_epoch_cn_en = len(cn_tensor_train)//BATCH_SIZE\n",
        "vocab_cn_size = len(cn_lang.word_index)+1\n",
        "vocab_en_cn_size = len(en_cn_lang.word_index)+1\n",
        "\n",
        "dataset_cn_en = tf.data.Dataset.from_tensor_slices((cn_tensor_train, en_cn_tensor_train)).shuffle(BUFFER_SIZE_CN)\n",
        "dataset_cn_en = dataset_cn_en.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uztunb8xe-S"
      },
      "source": [
        "encoder_cn = Encoder(vocab_cn_size, embedding_dim, units, BATCH_SIZE)\n",
        "attention_layer_cn = BahdanauAttention(10)\n",
        "decoder_cn = Decoder(vocab_en_cn_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpRrnbBIxe-S"
      },
      "source": [
        "checkpoint_dir = 'drive/MyDrive/Colab Notebooks/training_checkpoints_zh_en'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint_cn = tf.train.Checkpoint(optimizer=optimizer,encoder_cn=encoder_cn,decoder_cn=decoder_cn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbgVHmS8yXUe"
      },
      "source": [
        "@tf.function\n",
        "def train_step_zh_en(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder_cn(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([en_cn_lang.word_index['<s>']] * BATCH_SIZE, 1)\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder_cn(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder_cn.trainable_variables + decoder_cn.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7zfpYbRyXUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e902acc4-73f2-4479-b682-1e9ff89fc905"
      },
      "source": [
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder_cn.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset_cn_en.take(steps_per_epoch)):\n",
        "    batch_loss = train_step_zh_en(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint_cn.save(file_prefix = checkpoint_prefix)\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4476\n",
            "Epoch 1 Batch 100 Loss 2.3627\n",
            "Epoch 1 Batch 200 Loss 2.3417\n",
            "Epoch 1 Batch 300 Loss 2.2067\n",
            "Epoch 1 Loss 0.7347\n",
            "Time taken for 1 epoch 44.68572378158569 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9803\n",
            "Epoch 2 Batch 100 Loss 1.8171\n",
            "Epoch 2 Batch 200 Loss 1.7916\n",
            "Epoch 2 Batch 300 Loss 1.8280\n",
            "Epoch 2 Loss 0.5708\n",
            "Time taken for 1 epoch 27.40431547164917 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5292\n",
            "Epoch 3 Batch 100 Loss 1.6269\n",
            "Epoch 3 Batch 200 Loss 1.6787\n",
            "Epoch 3 Batch 300 Loss 1.7141\n",
            "Epoch 3 Loss 0.5014\n",
            "Time taken for 1 epoch 25.514168977737427 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5105\n",
            "Epoch 4 Batch 100 Loss 1.5352\n",
            "Epoch 4 Batch 200 Loss 1.4642\n",
            "Epoch 4 Batch 300 Loss 1.3486\n",
            "Epoch 4 Loss 0.4359\n",
            "Time taken for 1 epoch 26.194139003753662 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1443\n",
            "Epoch 5 Batch 100 Loss 1.2415\n",
            "Epoch 5 Batch 200 Loss 1.2172\n",
            "Epoch 5 Batch 300 Loss 1.3134\n",
            "Epoch 5 Loss 0.3726\n",
            "Time taken for 1 epoch 25.45593285560608 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0048\n",
            "Epoch 6 Batch 100 Loss 1.0998\n",
            "Epoch 6 Batch 200 Loss 0.9405\n",
            "Epoch 6 Batch 300 Loss 1.0433\n",
            "Epoch 6 Loss 0.3120\n",
            "Time taken for 1 epoch 26.243945598602295 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7830\n",
            "Epoch 7 Batch 100 Loss 0.7918\n",
            "Epoch 7 Batch 200 Loss 0.8096\n",
            "Epoch 7 Batch 300 Loss 0.8279\n",
            "Epoch 7 Loss 0.2549\n",
            "Time taken for 1 epoch 25.441632509231567 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6465\n",
            "Epoch 8 Batch 100 Loss 0.6308\n",
            "Epoch 8 Batch 200 Loss 0.6506\n",
            "Epoch 8 Batch 300 Loss 0.6849\n",
            "Epoch 8 Loss 0.2053\n",
            "Time taken for 1 epoch 27.178189754486084 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4792\n",
            "Epoch 9 Batch 100 Loss 0.4598\n",
            "Epoch 9 Batch 200 Loss 0.5572\n",
            "Epoch 9 Batch 300 Loss 0.5567\n",
            "Epoch 9 Loss 0.1628\n",
            "Time taken for 1 epoch 25.502599954605103 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3035\n",
            "Epoch 10 Batch 100 Loss 0.4246\n",
            "Epoch 10 Batch 200 Loss 0.3831\n",
            "Epoch 10 Batch 300 Loss 0.4292\n",
            "Epoch 10 Loss 0.1284\n",
            "Time taken for 1 epoch 26.27108669281006 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2980\n",
            "Epoch 11 Batch 100 Loss 0.3832\n",
            "Epoch 11 Batch 200 Loss 0.4515\n",
            "Epoch 11 Batch 300 Loss 0.4136\n",
            "Epoch 11 Loss 0.1028\n",
            "Time taken for 1 epoch 25.477779626846313 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1995\n",
            "Epoch 12 Batch 100 Loss 0.2041\n",
            "Epoch 12 Batch 200 Loss 0.2834\n",
            "Epoch 12 Batch 300 Loss 0.3272\n",
            "Epoch 12 Loss 0.0823\n",
            "Time taken for 1 epoch 26.239192724227905 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1814\n",
            "Epoch 13 Batch 100 Loss 0.2347\n",
            "Epoch 13 Batch 200 Loss 0.1936\n",
            "Epoch 13 Batch 300 Loss 0.2439\n",
            "Epoch 13 Loss 0.0693\n",
            "Time taken for 1 epoch 25.45109510421753 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1700\n",
            "Epoch 14 Batch 100 Loss 0.1705\n",
            "Epoch 14 Batch 200 Loss 0.1814\n",
            "Epoch 14 Batch 300 Loss 0.2041\n",
            "Epoch 14 Loss 0.0588\n",
            "Time taken for 1 epoch 27.095064163208008 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1255\n",
            "Epoch 15 Batch 100 Loss 0.1550\n",
            "Epoch 15 Batch 200 Loss 0.1945\n",
            "Epoch 15 Batch 300 Loss 0.2292\n",
            "Epoch 15 Loss 0.0514\n",
            "Time taken for 1 epoch 25.40922260284424 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0846\n",
            "Epoch 16 Batch 100 Loss 0.1158\n",
            "Epoch 16 Batch 200 Loss 0.1319\n",
            "Epoch 16 Batch 300 Loss 0.1811\n",
            "Epoch 16 Loss 0.0459\n",
            "Time taken for 1 epoch 26.0834379196167 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0860\n",
            "Epoch 17 Batch 100 Loss 0.1372\n",
            "Epoch 17 Batch 200 Loss 0.1388\n",
            "Epoch 17 Batch 300 Loss 0.1614\n",
            "Epoch 17 Loss 0.0419\n",
            "Time taken for 1 epoch 25.4577534198761 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1009\n",
            "Epoch 18 Batch 100 Loss 0.1384\n",
            "Epoch 18 Batch 200 Loss 0.1339\n",
            "Epoch 18 Batch 300 Loss 0.1633\n",
            "Epoch 18 Loss 0.0385\n",
            "Time taken for 1 epoch 26.166292428970337 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0871\n",
            "Epoch 19 Batch 100 Loss 0.0907\n",
            "Epoch 19 Batch 200 Loss 0.1477\n",
            "Epoch 19 Batch 300 Loss 0.1424\n",
            "Epoch 19 Loss 0.0358\n",
            "Time taken for 1 epoch 25.406259298324585 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0700\n",
            "Epoch 20 Batch 100 Loss 0.1127\n",
            "Epoch 20 Batch 200 Loss 0.1240\n",
            "Epoch 20 Batch 300 Loss 0.1527\n",
            "Epoch 20 Loss 0.0339\n",
            "Time taken for 1 epoch 26.660226345062256 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0824\n",
            "Epoch 21 Batch 100 Loss 0.0828\n",
            "Epoch 21 Batch 200 Loss 0.0970\n",
            "Epoch 21 Batch 300 Loss 0.1422\n",
            "Epoch 21 Loss 0.0330\n",
            "Time taken for 1 epoch 25.321820735931396 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0774\n",
            "Epoch 22 Batch 100 Loss 0.1215\n",
            "Epoch 22 Batch 200 Loss 0.0982\n",
            "Epoch 22 Batch 300 Loss 0.1093\n",
            "Epoch 22 Loss 0.0318\n",
            "Time taken for 1 epoch 26.034543991088867 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0703\n",
            "Epoch 23 Batch 100 Loss 0.0706\n",
            "Epoch 23 Batch 200 Loss 0.1177\n",
            "Epoch 23 Batch 300 Loss 0.1093\n",
            "Epoch 23 Loss 0.0303\n",
            "Time taken for 1 epoch 25.45104479789734 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0580\n",
            "Epoch 24 Batch 100 Loss 0.0818\n",
            "Epoch 24 Batch 200 Loss 0.1005\n",
            "Epoch 24 Batch 300 Loss 0.1246\n",
            "Epoch 24 Loss 0.0299\n",
            "Time taken for 1 epoch 27.12162733078003 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0536\n",
            "Epoch 25 Batch 100 Loss 0.0673\n",
            "Epoch 25 Batch 200 Loss 0.1493\n",
            "Epoch 25 Batch 300 Loss 0.0967\n",
            "Epoch 25 Loss 0.0286\n",
            "Time taken for 1 epoch 25.450319528579712 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0829\n",
            "Epoch 26 Batch 100 Loss 0.0747\n",
            "Epoch 26 Batch 200 Loss 0.1286\n",
            "Epoch 26 Batch 300 Loss 0.1452\n",
            "Epoch 26 Loss 0.0283\n",
            "Time taken for 1 epoch 26.186068296432495 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0614\n",
            "Epoch 27 Batch 100 Loss 0.0806\n",
            "Epoch 27 Batch 200 Loss 0.1147\n",
            "Epoch 27 Batch 300 Loss 0.1070\n",
            "Epoch 27 Loss 0.0278\n",
            "Time taken for 1 epoch 25.398940801620483 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0911\n",
            "Epoch 28 Batch 100 Loss 0.0825\n",
            "Epoch 28 Batch 200 Loss 0.1111\n",
            "Epoch 28 Batch 300 Loss 0.0972\n",
            "Epoch 28 Loss 0.0274\n",
            "Time taken for 1 epoch 26.241811990737915 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0489\n",
            "Epoch 29 Batch 100 Loss 0.1182\n",
            "Epoch 29 Batch 200 Loss 0.1210\n",
            "Epoch 29 Batch 300 Loss 0.1160\n",
            "Epoch 29 Loss 0.0264\n",
            "Time taken for 1 epoch 25.403725147247314 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0507\n",
            "Epoch 30 Batch 100 Loss 0.0642\n",
            "Epoch 30 Batch 200 Loss 0.1194\n",
            "Epoch 30 Batch 300 Loss 0.1090\n",
            "Epoch 30 Loss 0.0265\n",
            "Time taken for 1 epoch 27.125709533691406 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yd8v-DW0A5B"
      },
      "source": [
        "def evaluate_cn_en(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    string = ''\n",
        "    for i in sentence.split(' '):\n",
        "      try:\n",
        "        cn_lang.word_index[i]\n",
        "        string=string+i+' '\n",
        "        # print (string)\n",
        "      except Exception:\n",
        "        string=string+' OOV'\n",
        "        print (string)\n",
        "    sentence = string \n",
        "    print(sentence)\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [cn_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder_cn(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([en_cn_lang.word_index['<s>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder_cn(dec_input, dec_hidden, enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += en_cn_lang.index_word[predicted_id] + ' '\n",
        "        if en_cn_lang.index_word[predicted_id] == '<s/>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJbXl5Cx0A5C"
      },
      "source": [
        "def translate_cn2en(sentence):\n",
        "    result, sentence,_ = evaluate_cn_en(sentence)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtfOujQV1Mdv"
      },
      "source": [
        "print(translate_cn2en(u\"我喜欢你。\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6CAHdpHt1Q8",
        "outputId": "828f7160-70a2-4d16-af9d-e76ec461725a"
      },
      "source": [
        "#BLEU score of Chinese English Translation\n",
        "score = 0\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "for i in range(100,200):\n",
        "\n",
        "  # sentence = cleanSentence(mono_en[i])\n",
        "  sentence = merge(cn_lang,cn_tensor_val[i]).strip()\n",
        "  # print(sentence)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  sentence_cn = strinfo.sub('', sentence)\n",
        "  sentence_en = translate_cn2en(sentence_cn)\n",
        "  result = strinfo.sub('', sentence_en)\n",
        "  # print(result)\n",
        "  result = result.split()\n",
        "  reference = [merge(en_cn_lang,en_cn_tensor_val[i]).strip().split()]\n",
        "\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5687710974378809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WRV5w1Pt1o2",
        "outputId": "9797f3dc-81fd-4ce4-ce76-55094ba31d25"
      },
      "source": [
        "#BLEU score of Chinese English Translation\n",
        "score = 0\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "for i in range(110):\n",
        "\n",
        "  sentence = cleanSentence(mono_zh[i])\n",
        "  # sentence = merge(en_de_lang,sentence).strip()\n",
        "  # print(sentence)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  sentence_de = strinfo.sub('', sentence)\n",
        "  sentence_de = translate_cn2en(sentence)\n",
        "  result = strinfo.sub('', sentence_de)\n",
        "  # print(result)\n",
        "  result = result.split()\n",
        "  reference = [cleanSentence(mono_en[i]).strip().split()]\n",
        "\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/110)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.49547567249833757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "wtt4WDVMxSdS",
        "outputId": "e029c9b5-acb4-4c9c-9495-5c866d7bc916"
      },
      "source": [
        "translate_cn2en('我 在 我 的 实 验 室 裏 。')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "我 在 我 的 实 验 室 裏 。 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i'm talking in my kitchen . <s/> \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iMK40XnHyL8D",
        "outputId": "7f01118c-a93f-4859-d40f-4f5b2e96f4cd"
      },
      "source": [
        "translate(u'i chicken you like like .')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich möchte ihnen wohl so wie du magst . <s/> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YecHdxxSDVIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcNVOMolf8ok"
      },
      "source": [
        "# Compute the mapping matrix\n",
        "def model (w,matrix_source):\n",
        "  return matrix_source@w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbZjoWw_ggz2"
      },
      "source": [
        "def cost_function(w,matrix_source,matrix_target):\n",
        "    n = 64\n",
        "    return 0.5/n * (np.square(matrix_target-matrix_source@w)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLtZJKehhaMt"
      },
      "source": [
        "def optimize(w,matrix_source,matrix_target):\n",
        "    n = 64\n",
        "    alpha = 5*1e-2\n",
        "    y_hat = model(w,matrix_source)\n",
        "    da = (1.0/n) * ((y_hat-matrix_target)*matrix_source).sum()\n",
        "    w = w - alpha*da\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVFf22j5qSOl"
      },
      "source": [
        "def iterate(w,matrix_source,matrix_target,times):\n",
        "    for i in range(times):\n",
        "        w = optimize(w,matrix_source,matrix_target)\n",
        "\n",
        "    y_hat=model(w,matrix_source)\n",
        "    cost = cost_function(w,matrix_source,matrix_target)\n",
        "    print(w,cost)\n",
        "\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkAoAJGfqvqH",
        "outputId": "92927574-2807-408f-e2c5-59c5267d60c8"
      },
      "source": [
        "for i in context_vector_cn:\n",
        "  w = iterate(w,context_vector_cn.numpy()[i],context_vector_en_cn.numpy()[i],10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00359747 0.02975853 0.07078533 ... 0.02138552 0.08781181 0.04771207]\n",
            " [0.03562495 0.04025518 0.0523388  ... 0.02231064 0.06145953 0.0107934 ]\n",
            " [0.00963287 0.0963925  0.02958895 ... 0.02512286 0.01474058 0.02905766]\n",
            " ...\n",
            " [0.03495453 0.0611114  0.00345175 ... 0.02899886 0.03915337 0.05887099]\n",
            " [0.05802387 0.06030348 0.02565435 ... 0.07376109 0.0143155  0.00013901]\n",
            " [0.01526998 0.03803491 0.00755802 ... 0.01191425 0.06561932 0.02601225]] 0.05662886224731087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvW3GvMRFiYk"
      },
      "source": [
        "def evaluate_tsf_cn_en(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    string = ''\n",
        "    for i in sentence.split(' '):\n",
        "      try:\n",
        "        cn_lang.word_index[i]\n",
        "        string=string+i+' '\n",
        "        # print (string)\n",
        "      except Exception:\n",
        "        string=string+' OOV'\n",
        "        print (string)\n",
        "    sentence = string \n",
        "    print(sentence)\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    # print(sentence)\n",
        "    inputs = [cn_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    # print(inputs)\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out_cn, enc_hidden = encoder_cn(inputs, hidden)\n",
        "    enc_out_cn_tsf = enc_out_cn@w\n",
        "    enc_hidden = enc_out_cn_tsf[-1] \n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([en_cn_lang.word_index['<s>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder_cn(dec_input, dec_hidden, enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += en_cn_lang.index_word[predicted_id] + ' '\n",
        "        if en_cn_lang.index_word[predicted_id] == '<s/>':\n",
        "            return result, sentence, attention_plot\n",
        "        # 预测的 ID 被输送回模型\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkfTdzF2kOK"
      },
      "source": [
        "def translate_cn2de(sentence):\n",
        "  s = translate_cn2en(sentence)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  sentence_en = strinfo.sub('', s).strip()\n",
        "  print(sentence_en)\n",
        "  result = translate(sentence_en)\n",
        "  \n",
        "  # print (result)\n",
        "  strinfo = re.compile('<s/>')\n",
        "  result = strinfo.sub('', result).strip()\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}