{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpe_translation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4LgKitBexr"
      },
      "source": [
        "# Chinese English Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhYvEwrBrZwR",
        "outputId": "2ccbd67e-89ac-43f4-a56f-2a528101597c"
      },
      "source": [
        "pip install opencc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (765 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 440 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 450 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 460 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 471 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 501 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 512 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 522 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 532 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 542 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 563 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 573 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 583 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 593 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 604 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 614 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 634 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 645 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 655 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 665 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 675 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 686 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 706 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 716 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 727 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 737 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 747 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 757 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 765 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cHZTxYR2ozn",
        "outputId": "0e487c48-06db-4678-a4dd-ab8b1c5c320a"
      },
      "source": [
        "pip install bpemb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.62.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHmycF4_reiM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import jieba\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from opencc import OpenCC\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2dxtBhG2swI",
        "outputId": "8331ec37-74ab-43b8-8ca3-6b46cc0ff8ce"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\", dim=100,vs=100000)\n",
        "bpemb_zh = BPEmb(lang=\"zh\", dim=100,vs=100000)\n",
        "bpemb_de = BPEmb(lang=\"de\", dim=100,vs=100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1987533/1987533 [00:00<00:00, 2879840.54B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37969196/37969196 [00:02<00:00, 14913049.29B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1849493/1849493 [00:00<00:00, 3073755.81B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37914051/37914051 [00:02<00:00, 17706096.45B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/de/de.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2068307/2068307 [00:00<00:00, 3436210.40B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/de/de.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 38035802/38035802 [00:02<00:00, 15588865.01B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfm67sYergIn"
      },
      "source": [
        "class Encoder_cn(tf.keras.Model):\n",
        "  def __init__(self, enc_units, batch_sz):\n",
        "    super(Encoder_cn, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(100000, 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTo712sRolCa"
      },
      "source": [
        "class Encoder_en_de(tf.keras.Model):\n",
        "  def __init__(self, enc_units, batch_sz):\n",
        "    super(Encoder_en_de, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(100000, 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIpwb6dfriDy"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0EdmG4zrj5W"
      },
      "source": [
        "class Decoder_en(tf.keras.Model):\n",
        "  def __init__(self,  dec_units, batch_sz):\n",
        "    super(Decoder_en, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(100000, 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(100000)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3f_J2VNosfj"
      },
      "source": [
        "class Decoder_de(tf.keras.Model):\n",
        "  def __init__(self,  dec_units, batch_sz):\n",
        "    super(Decoder_de, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(100000, 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(100000)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlG2S7b-sxuB"
      },
      "source": [
        "d_model = 128\n",
        "# d_model = train_step_en_de(inp, targ, enc_hidden)\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=6000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih2LNdeEszMF"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.003, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gqQiuxFs0cd",
        "outputId": "3eaefc96-ff08-4d94-ec18-b48d0e4e0dc2"
      },
      "source": [
        "path_to_file = \"drive/MyDrive/Colab Notebooks/cmn-clean.txt\"\n",
        "path_to_file_de = \"drive/MyDrive/Colab Notebooks/deu.txt\"\n",
        "path_to_singleCorpus = \"drive/MyDrive/Colab Notebooks/single_corpus_zh.txt\"\n",
        "path_to_newCorpus = \"drive/MyDrive/Colab Notebooks/new_corpus.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "input_file = open(path_to_file,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_nIZ506dw7"
      },
      "source": [
        "cc = OpenCC('t2s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpO5U913SAp"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "    w = cc.convert(w)\n",
        "    w = re.sub(r\"[^\\u4e00-\\u9fa5\\u0080-\\uFFFF_a-zA-Z0-9?.!,:：？。，！']+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "    w = '| ' + w + ' /'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P576euRT6ZgB"
      },
      "source": [
        "def create_dataset(path):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines]\n",
        "    # print(word_pairs[200])\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2qhP8_0y2Bb"
      },
      "source": [
        "# def max_length(tensor):\n",
        "#     return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ws2SDaXy-Jc"
      },
      "source": [
        "def uniformLength_en_cn(list1,list2,maxlen,minlen):\n",
        "  print(len(list1),len(list2))\n",
        "  length = len(list1)\n",
        "  a= 0\n",
        "  l =0\n",
        "  while length != l:\n",
        "    l = len(list1)\n",
        "    for i in range(a,length):\n",
        "      if (len(bpemb_en.encode_ids(list1[i]))> maxlen or len(bpemb_en.encode_ids(list1[i]))< minlen\n",
        "          or len(bpemb_zh.encode_ids(list2[i]))> maxlen or len(bpemb_zh.encode_ids(list2[i]))< minlen):\n",
        "        del list1[i]\n",
        "        del list2[i]\n",
        "        length = len(list2)\n",
        "        a=i\n",
        "        break\n",
        "      else:\n",
        "        list1[i] = list1[i] + ' pad'*(maxlen-len(bpemb_en.encode_ids(list1[i])))\n",
        "        list2[i] = list2[i] + ' pad'*(maxlen-len(bpemb_zh.encode_ids(list2[i])))\n",
        "    \n",
        "  # print(len(list1),len(list2))\n",
        "  # print(list2[100])\n",
        "  return list2,list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2xHJ9sG3msq"
      },
      "source": [
        "def cutDataset_en_cn(targ_lang,inp_lang,maxlen,minlen):\n",
        "    targ_lang=list(targ_lang)\n",
        "    inp_lang=list(inp_lang)\n",
        "    targ_lang, inp_lang =uniformLength_en_cn(targ_lang, inp_lang,maxlen,minlen)\n",
        "    return inp_lang, targ_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "479q5eZ53qca"
      },
      "source": [
        "def uniformLength_en_de(list1,list2,maxlen,minlen):\n",
        "  print(len(list1),len(list2))\n",
        "  length = len(list1)\n",
        "  a= 0\n",
        "  l =0\n",
        "  while length != l:\n",
        "    l = len(list1)\n",
        "    for i in range(a,length):\n",
        "      if (len(bpemb_en.encode_ids(list1[i]))> maxlen or len(bpemb_en.encode_ids(list1[i]))< minlen\n",
        "          or len(bpemb_de.encode_ids(list2[i]))> maxlen or len(bpemb_de.encode_ids(list2[i]))< minlen):\n",
        "        del list1[i]\n",
        "        del list2[i]\n",
        "        length = len(list2)\n",
        "        a=i\n",
        "        break\n",
        "      else:\n",
        "        list1[i] = list1[i] + ' pad'*(maxlen-len(bpemb_en.encode_ids(list1[i])))\n",
        "        list2[i] = list2[i] + ' pad'*(maxlen-len(bpemb_de.encode_ids(list2[i]))\n",
        "  # print(len(list1),len(list2))\n",
        "  return list2,list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OWuEXZT3r6K"
      },
      "source": [
        "def cutDataset_en_de(targ_lang,inp_lang,maxlen,minlen):\n",
        "    targ_lang=list(targ_lang)\n",
        "    inp_lang=list(inp_lang)\n",
        "    targ_lang, inp_lang =uniformLength_en_de(targ_lang, inp_lang,maxlen,minlen)\n",
        "    return inp_lang, targ_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3FNUfq4Lwu",
        "outputId": "f1ae4e85-8124-4fd1-de82-07804d4cfd29"
      },
      "source": [
        "en_cn, cn = create_dataset(path_to_file)\n",
        "en_cn, cn =cutDataset_en_cn(en_cn, cn,15,1)\n",
        "en_de, de = create_dataset(path_to_file_de)\n",
        "en_de, de =cutDataset_en_de(en_de, de,15,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26828 26828\n",
            "240828 240828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuvLWNpSVAs"
      },
      "source": [
        "cn_enc=bpemb_zh.encode_ids(cn[0])\n",
        "for i in range(1,len(cn)):\n",
        "  a = bpemb_zh.encode_ids(cn[i])\n",
        "  cn_enc= np.append(cn_enc,a)\n",
        "cn_enc = cn_enc.reshape(len(cn),15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIkz8ZSDaLOW"
      },
      "source": [
        "en_enc=bpemb_en.encode_ids(en_cn[0])\n",
        "for i in range(1,len(en_cn)):\n",
        "  a = bpemb_en.encode_ids(en_cn[i])\n",
        "  en_enc= np.append(en_enc,a)\n",
        "en_enc = en_enc.reshape(len(en_cn),15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no_s8_rkzDMl",
        "outputId": "d486a785-9a5b-4f79-d812-3afa726e960d"
      },
      "source": [
        "en_cn_tensor_train, en_cn_tensor_val, cn_tensor_train, cn_tensor_val = train_test_split(en_enc, cn_enc, test_size=0.1)\n",
        "print(len(en_cn_tensor_train), len(cn_tensor_train), len(en_cn_tensor_val), len(cn_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22995 22995 2555 2555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4SE5Xd06wTz"
      },
      "source": [
        "BUFFER_SIZE = len(cn_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(cn_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "dataset_cn2en = tf.data.Dataset.from_tensor_slices((cn_tensor_train, en_cn_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset_cn2en = dataset_cn2en.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ_ieHQD4hjP"
      },
      "source": [
        "encoder_bpe_cn = Encoder_cn(units, BATCH_SIZE)\n",
        "attention_layer_bpe_cn = BahdanauAttention(10)\n",
        "decoder_bpe_en = Decoder_en(units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_zZwQHR4lxD"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_bpe_cn2en'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint_bpe_cn2en = tf.train.Checkpoint(optimizer=optimizer, encoder_bpe_cn=encoder_bpe_cn, decoder_bpe_en=decoder_bpe_en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5oFWjAi5QA2"
      },
      "source": [
        "@tf.function\n",
        "def train_step_bpe_zh_en(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # print('start')\n",
        "    enc_output, enc_hidden = encoder_bpe_cn(inp, enc_hidden)\n",
        "    # print('end')\n",
        "    # print(enc_output.shape)\n",
        "    # print(enc_output)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(bpemb_en.encode_ids(\"|\") * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "\n",
        "      predictions, dec_hidden, _ = decoder_bpe_en(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # print('taching')\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder_bpe_cn.trainable_variables + decoder_bpe_en.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_DNLZSq5_bC",
        "outputId": "eb20cb87-0ab6-44f7-b5af-02614fc6f333"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder_bpe_cn.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset_cn2en.take(steps_per_epoch)):\n",
        "    # print(type(inp))\n",
        "    # print(batch)\n",
        "    batch_loss = train_step_bpe_zh_en(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint_bpe_cn2en.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.7454\n",
            "Epoch 1 Batch 100 Loss 3.7534\n",
            "Epoch 1 Batch 200 Loss 3.2391\n",
            "Epoch 1 Batch 300 Loss 3.0993\n",
            "Epoch 1 Loss 3.5655\n",
            "Time taken for 1 epoch 175.94539070129395 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.9395\n",
            "Epoch 2 Batch 100 Loss 2.9454\n",
            "Epoch 2 Batch 200 Loss 2.2026\n",
            "Epoch 2 Batch 300 Loss 2.3377\n",
            "Epoch 2 Loss 2.4857\n",
            "Time taken for 1 epoch 159.26229667663574 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9486\n",
            "Epoch 3 Batch 100 Loss 1.9484\n",
            "Epoch 3 Batch 200 Loss 1.9432\n",
            "Epoch 3 Batch 300 Loss 1.9120\n",
            "Epoch 3 Loss 1.8914\n",
            "Time taken for 1 epoch 152.03226351737976 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6481\n",
            "Epoch 4 Batch 100 Loss 1.6310\n",
            "Epoch 4 Batch 200 Loss 1.4776\n",
            "Epoch 4 Batch 300 Loss 1.7743\n",
            "Epoch 4 Loss 1.5887\n",
            "Time taken for 1 epoch 159.1578299999237 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2873\n",
            "Epoch 5 Batch 100 Loss 1.3381\n",
            "Epoch 5 Batch 200 Loss 1.2909\n",
            "Epoch 5 Batch 300 Loss 1.2061\n",
            "Epoch 5 Loss 1.3326\n",
            "Time taken for 1 epoch 151.64581632614136 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0657\n",
            "Epoch 6 Batch 100 Loss 0.8329\n",
            "Epoch 6 Batch 200 Loss 1.1587\n",
            "Epoch 6 Batch 300 Loss 1.3164\n",
            "Epoch 6 Loss 1.1090\n",
            "Time taken for 1 epoch 158.23026371002197 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9513\n",
            "Epoch 7 Batch 100 Loss 0.8581\n",
            "Epoch 7 Batch 200 Loss 0.8772\n",
            "Epoch 7 Batch 300 Loss 0.9479\n",
            "Epoch 7 Loss 0.9152\n",
            "Time taken for 1 epoch 151.59115076065063 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7094\n",
            "Epoch 8 Batch 100 Loss 0.7833\n",
            "Epoch 8 Batch 200 Loss 0.8684\n",
            "Epoch 8 Batch 300 Loss 0.7569\n",
            "Epoch 8 Loss 0.7469\n",
            "Time taken for 1 epoch 158.0609188079834 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5365\n",
            "Epoch 9 Batch 100 Loss 0.4832\n",
            "Epoch 9 Batch 200 Loss 0.6969\n",
            "Epoch 9 Batch 300 Loss 0.7701\n",
            "Epoch 9 Loss 0.6099\n",
            "Time taken for 1 epoch 151.34733295440674 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4346\n",
            "Epoch 10 Batch 100 Loss 0.5242\n",
            "Epoch 10 Batch 200 Loss 0.5572\n",
            "Epoch 10 Batch 300 Loss 0.5718\n",
            "Epoch 10 Loss 0.5003\n",
            "Time taken for 1 epoch 162.932110786438 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3170\n",
            "Epoch 11 Batch 100 Loss 0.3470\n",
            "Epoch 11 Batch 200 Loss 0.3975\n",
            "Epoch 11 Batch 300 Loss 0.4699\n",
            "Epoch 11 Loss 0.4109\n",
            "Time taken for 1 epoch 151.07491850852966 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2776\n",
            "Epoch 12 Batch 100 Loss 0.3119\n",
            "Epoch 12 Batch 200 Loss 0.4302\n",
            "Epoch 12 Batch 300 Loss 0.4275\n",
            "Epoch 12 Loss 0.3427\n",
            "Time taken for 1 epoch 158.11703610420227 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2340\n",
            "Epoch 13 Batch 100 Loss 0.2614\n",
            "Epoch 13 Batch 200 Loss 0.3312\n",
            "Epoch 13 Batch 300 Loss 0.2944\n",
            "Epoch 13 Loss 0.2863\n",
            "Time taken for 1 epoch 151.53541040420532 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1785\n",
            "Epoch 14 Batch 100 Loss 0.2499\n",
            "Epoch 14 Batch 200 Loss 0.2457\n",
            "Epoch 14 Batch 300 Loss 0.2998\n",
            "Epoch 14 Loss 0.2437\n",
            "Time taken for 1 epoch 158.75536608695984 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1848\n",
            "Epoch 15 Batch 100 Loss 0.2045\n",
            "Epoch 15 Batch 200 Loss 0.2146\n",
            "Epoch 15 Batch 300 Loss 0.1868\n",
            "Epoch 15 Loss 0.2109\n",
            "Time taken for 1 epoch 150.8890130519867 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1333\n",
            "Epoch 16 Batch 100 Loss 0.1404\n",
            "Epoch 16 Batch 200 Loss 0.1993\n",
            "Epoch 16 Batch 300 Loss 0.1971\n",
            "Epoch 16 Loss 0.1864\n",
            "Time taken for 1 epoch 157.02192854881287 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1230\n",
            "Epoch 17 Batch 100 Loss 0.1581\n",
            "Epoch 17 Batch 200 Loss 0.1460\n",
            "Epoch 17 Batch 300 Loss 0.1881\n",
            "Epoch 17 Loss 0.1662\n",
            "Time taken for 1 epoch 149.29135823249817 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1186\n",
            "Epoch 18 Batch 100 Loss 0.1576\n",
            "Epoch 18 Batch 200 Loss 0.1355\n",
            "Epoch 18 Batch 300 Loss 0.1696\n",
            "Epoch 18 Loss 0.1518\n",
            "Time taken for 1 epoch 156.95647263526917 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1059\n",
            "Epoch 19 Batch 100 Loss 0.1130\n",
            "Epoch 19 Batch 200 Loss 0.1694\n",
            "Epoch 19 Batch 300 Loss 0.1984\n",
            "Epoch 19 Loss 0.1402\n",
            "Time taken for 1 epoch 148.94454622268677 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0943\n",
            "Epoch 20 Batch 100 Loss 0.1194\n",
            "Epoch 20 Batch 200 Loss 0.1085\n",
            "Epoch 20 Batch 300 Loss 0.1467\n",
            "Epoch 20 Loss 0.1324\n",
            "Time taken for 1 epoch 156.51124000549316 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0999\n",
            "Epoch 21 Batch 100 Loss 0.0889\n",
            "Epoch 21 Batch 200 Loss 0.0963\n",
            "Epoch 21 Batch 300 Loss 0.1346\n",
            "Epoch 21 Loss 0.1242\n",
            "Time taken for 1 epoch 149.46139526367188 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0836\n",
            "Epoch 22 Batch 100 Loss 0.1038\n",
            "Epoch 22 Batch 200 Loss 0.1577\n",
            "Epoch 22 Batch 300 Loss 0.1111\n",
            "Epoch 22 Loss 0.1181\n",
            "Time taken for 1 epoch 156.7105782032013 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0720\n",
            "Epoch 23 Batch 100 Loss 0.1080\n",
            "Epoch 23 Batch 200 Loss 0.0913\n",
            "Epoch 23 Batch 300 Loss 0.1307\n",
            "Epoch 23 Loss 0.1136\n",
            "Time taken for 1 epoch 152.14429593086243 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0817\n",
            "Epoch 24 Batch 100 Loss 0.0975\n",
            "Epoch 24 Batch 200 Loss 0.1104\n",
            "Epoch 24 Batch 300 Loss 0.1384\n",
            "Epoch 24 Loss 0.1088\n",
            "Time taken for 1 epoch 158.87466859817505 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0707\n",
            "Epoch 25 Batch 100 Loss 0.0913\n",
            "Epoch 25 Batch 200 Loss 0.1019\n",
            "Epoch 25 Batch 300 Loss 0.1222\n",
            "Epoch 25 Loss 0.1036\n",
            "Time taken for 1 epoch 151.64671111106873 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH35_jTVJ0Rn",
        "outputId": "010f0c2c-911a-493a-9feb-ba350250e37d"
      },
      "source": [
        "checkpoint_bpe_cn2en.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fd8a695a590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ6S7zdc_wDo"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    # attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sen = preprocess_sentence(sentence)\n",
        "    if len(bpemb_zh.encode_ids(sen))<15:\n",
        "      sen = sen + ' pad'*(15-len(bpemb_zh.encode_ids(sen)))\n",
        "    sen_enc = [bpemb_zh.encode_ids(sen)]\n",
        "    inputs = tf.convert_to_tensor(sen_enc)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder_bpe_cn(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([10386] , 0)\n",
        "\n",
        "    for t in range(15):\n",
        "        predictions, dec_hidden, attention_weights = decoder_bpe_en(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # store attention weights to plot attention figures\n",
        "        # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        # attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += bpemb_en.decode_ids([int(predicted_id)]) + ' '\n",
        "\n",
        "        if predicted_id == 2781:\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJEyjXVpAJKE"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BCwuaRtJDp3"
      },
      "source": [
        "sen = (u'我明天早上回家。')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06H3WX5yJFn3"
      },
      "source": [
        "translate(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcNVOMolf8ok"
      },
      "source": [
        "def model (w,matrix_source):\n",
        "  return matrix_source@w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbZjoWw_ggz2"
      },
      "source": [
        "def cost_function(w,matrix_source,matrix_target):\n",
        "    n = 64\n",
        "    return 0.5/n * (np.square(matrix_target-matrix_source@w)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLtZJKehhaMt"
      },
      "source": [
        "def optimize(w,matrix_source,matrix_target):\n",
        "    n = 64\n",
        "    alpha = 5*1e-2\n",
        "    y_hat = model(w,matrix_source)\n",
        "    da = (1.0/n) * ((y_hat-matrix_target)*matrix_source).sum()\n",
        "    w = w - alpha*da\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVFf22j5qSOl"
      },
      "source": [
        "def iterate(w,matrix_source,matrix_target,times):\n",
        "    for i in range(times):\n",
        "        w = optimize(w,matrix_source,matrix_target)\n",
        "\n",
        "    y_hat=model(w,matrix_source)\n",
        "    cost = cost_function(w,matrix_source,matrix_target)\n",
        "    print(w,cost)\n",
        "\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkAoAJGfqvqH",
        "outputId": "92927574-2807-408f-e2c5-59c5267d60c8"
      },
      "source": [
        "for i in context_vector_cn:\n",
        "  w = iterate(w,context_vector_cn.numpy()[i],context_vector_en_cn.numpy()[i],10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00359747 0.02975853 0.07078533 ... 0.02138552 0.08781181 0.04771207]\n",
            " [0.03562495 0.04025518 0.0523388  ... 0.02231064 0.06145953 0.0107934 ]\n",
            " [0.00963287 0.0963925  0.02958895 ... 0.02512286 0.01474058 0.02905766]\n",
            " ...\n",
            " [0.03495453 0.0611114  0.00345175 ... 0.02899886 0.03915337 0.05887099]\n",
            " [0.05802387 0.06030348 0.02565435 ... 0.07376109 0.0143155  0.00013901]\n",
            " [0.01526998 0.03803491 0.00755802 ... 0.01191425 0.06561932 0.02601225]] 0.05662886224731087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPuKMzA7dwLe"
      },
      "source": [
        "# Translate English to German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6EaesiFj-5L"
      },
      "source": [
        "en_cn, cn = create_dataset(path_to_file)\n",
        "en_cn, cn =cutDataset_en_cn(en_cn, cn,15,1)\n",
        "en_de, de = create_dataset(path_to_file_de)\n",
        "en_de, de =cutDataset_en_de(en_de, de,15,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxP_UhNSj4Zs"
      },
      "source": [
        "de_enc=bpemb_de.encode_ids(de[0])\n",
        "for i in range(1,120000):\n",
        "# for i in range(1,10):\n",
        "  a = bpemb_de.encode_ids(de[i])\n",
        "  de_enc= np.append(de_enc,a)\n",
        "de_enc = de_enc.reshape(120000,15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A873pwkUj4Z3"
      },
      "source": [
        "en_de_enc=bpemb_en.encode_ids(en_de[0])\n",
        "for i in range(1,120000):\n",
        "# for i in range(1,10):\n",
        "  a = bpemb_en.encode_ids(en_de[i])\n",
        "  en_de_enc= np.append(en_de_enc,a)\n",
        "en_de_enc = en_de_enc.reshape(120000,15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfbCXxvGmPfh"
      },
      "source": [
        "# en_cn_tensor_train, en_cn_tensor_val, cn_tensor_train, cn_tensor_val = train_test_split(en_enc, cn_enc, test_size=0.1)\n",
        "print(len(en_de_tensor_train), len(de_tensor_train), len(en_de_tensor_val), len(de_tensor_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDAR-WDQm2t3"
      },
      "source": [
        "BUFFER_SIZE = len(en_de_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(en_de_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPXwtsX9n1HX"
      },
      "source": [
        "dataset_en2de = tf.data.Dataset.from_tensor_slices((en_de_tensor_train, de_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset_en2de = dataset_en2de.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG2Pf826oY5n"
      },
      "source": [
        "encoder_bpe_en_de = Encoder_en_de(units, BATCH_SIZE)\n",
        "attention_layer_bpe_cn = BahdanauAttention(10)\n",
        "decoder_bpe_de = Decoder_de(units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8Z-3xYo4Wx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57iKMfJ7pBEZ"
      },
      "source": [
        "checkpoint_dir = './training_checkpoint_bpe_en2de'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint_bpe_en2de = tf.train.Checkpoint(optimizer=optimizer, encoder_bpe_en_de=encoder_bpe_en_de, decoder_bpe_de=decoder_bpe_de)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JDPmMNwsdJm"
      },
      "source": [
        "bpemb_de.encode_ids(\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_iw4pgipBEa"
      },
      "source": [
        "@tf.function\n",
        "def train_step_bpe_en_de(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # print('start')\n",
        "    enc_output, enc_hidden = encoder_bpe_en_de(inp, enc_hidden)\n",
        "    # print('end')\n",
        "    # print(enc_output.shape)\n",
        "    # print(enc_output)\n",
        "    dec_hidden = enc_hidden\n",
        "    # dec_input = dec_inp\n",
        "\n",
        "    dec_input = tf.expand_dims(bpemb_de.encode_ids(\"|\") * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder_bpe_de(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # print('taching')\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder_bpe_en_de.trainable_variables + decoder_bpe_de.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBVmtUdepBEa"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder_bpe_en_de.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset_en2de.take(steps_per_epoch)):\n",
        "    # print(type(inp))\n",
        "    # print(batch)\n",
        "    batch_loss = train_step_bpe_en_de(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint_bpe_en2de.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34P2INopBEc"
      },
      "source": [
        "checkpoint_bpe_en2de.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UAKJfWjpBEd"
      },
      "source": [
        "def evaluate_en2de(sentence):\n",
        "    sen = preprocess_sentence(sentence)\n",
        "    if len(bpemb_en.encode_ids(sen))<15:\n",
        "      sen = sen + ' pad'*(15-len(bpemb_en.encode_ids(sen)))\n",
        "    sen_enc = [bpemb_en.encode_ids(sen)]\n",
        "    # print(sen_enc)\n",
        "    inputs = tf.convert_to_tensor(sen_enc)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder_bpe_en_de(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([7641] , 0)\n",
        "\n",
        "    for t in range(15):\n",
        "        predictions, dec_hidden, attention_weights = decoder_bpe_de(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += bpemb_de.decode_ids([int(predicted_id)]) + ' '\n",
        "\n",
        "        if predicted_id == 1914:\n",
        "            return result, sentence\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu3ZAmBghs-Q"
      },
      "source": [
        "def translate_en2de(sentence):\n",
        "    result, sentence = evaluate_en2de(sentence)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl-ERYFstmbT"
      },
      "source": [
        "def cleanSentence(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  strinfo1 = re.compile('\\|')\n",
        "  sentence = strinfo1.sub('', sentence).strip()\n",
        "  strinfo2 = re.compile('/')\n",
        "  sentence = strinfo2.sub('', sentence).strip()\n",
        "  strinfo3 = re.compile('pad')\n",
        "  sentence = strinfo3.sub('', sentence).strip()\n",
        "  return sentence \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgF6SkThqzfo"
      },
      "source": [
        "#BLEU score of English German Translation\n",
        "score = 0\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "for i in range(100,200):\n",
        "  s = en_de_tensor_val[i]\n",
        "\n",
        "  sentence = bpemb_en.decode_ids(s)\n",
        "  print(sentence)\n",
        "  strinfo1 = re.compile('\\|')\n",
        "  sentence = strinfo1.sub('', sentence).strip()\n",
        "  strinfo2 = re.compile('/')\n",
        "  sentence = strinfo2.sub('', sentence).strip()\n",
        "  strinfo3 = re.compile('pad')\n",
        "  sentence = strinfo3.sub('', sentence).strip()\n",
        "  sentence_de = translate_en2de(sentence)\n",
        "  sentence_de = strinfo2.sub('', sentence_de).strip()\n",
        "  result = sentence_de\n",
        "  # print(result)\n",
        "  result = result.split()\n",
        "  # print(de_tensor_val[i])\n",
        "  sen_val = bpemb_de.decode_ids(de_tensor_val[i])\n",
        "  # print(sen_val)\n",
        "  reference = [cleanSentence(sen_val).split()]\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOn7DP4tyCbk"
      },
      "source": [
        "#BLEU score of Chinese English Translation\n",
        "score = 0\n",
        "for i in range(100,300):\n",
        "  s = cn_tensor_val[i]\n",
        "  sentence = bpemb_zh.decode_ids(s)\n",
        "  strinfo1 = re.compile('\\|')\n",
        "  sentence = strinfo1.sub('', sentence).strip()\n",
        "  strinfo2 = re.compile('/')\n",
        "  sentence = strinfo2.sub('', sentence).strip()\n",
        "  strinfo3 = re.compile('pad')\n",
        "  sentence = strinfo3.sub('', sentence).strip()\n",
        "  sentence_cn2en = translate(sentence)\n",
        "  sentence_cn2en = strinfo2.sub('', sentence_cn2en).strip()\n",
        "  result = sentence_cn2en.split()\n",
        "  sen_val = bpemb_en.decode_ids(en_cn_tensor_val[i])\n",
        "  reference = [cleanSentence(sen_val).split()]\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOw2DI2xtWUO"
      },
      "source": [
        "sen_val = bpemb_de.decode_ids(de_tensor_val[i])\n",
        "reference = [cleanSentence(sen_val).split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XehowcrSuDUb"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "score = sentence_bleu(reference, result)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmT5eOFq1Zef"
      },
      "source": [
        "path_mono_de_test = \"drive/MyDrive/Colab Notebooks/single_corpus_de_test.txt\"\n",
        "path_mono_en_test = \"drive/MyDrive/Colab Notebooks/single_corpus_en_test.txt\"\n",
        "path_mono_zh_test = \"drive/MyDrive/Colab Notebooks/single_corpus_zh_test.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "file_mono_de_test = open(path_mono_de_test,\"r\",encoding=\"utf-8\")\n",
        "file_mono_en_test = open(path_mono_en_test,\"r\",encoding=\"utf-8\")\n",
        "file_mono_zh_test = open(path_mono_zh_test,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20V0x1MgLX5b"
      },
      "source": [
        "path_mono_de = \"drive/MyDrive/Colab Notebooks/single_corpus_de.txt\"\n",
        "path_mono_en = \"drive/MyDrive/Colab Notebooks/single_corpus_en.txt\"\n",
        "path_mono_zh = \"drive/MyDrive/Colab Notebooks/single_corpus_zh.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "file_mono_de = open(path_mono_de,\"r\",encoding=\"utf-8\")\n",
        "file_mono_en = open(path_mono_en,\"r\",encoding=\"utf-8\")\n",
        "file_mono_zh = open(path_mono_zh,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOoMrf-z3MmU"
      },
      "source": [
        "def openFile(path):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # for line in lines:\n",
        "  #   line = preprocess_sentence(line)\n",
        "\n",
        "    # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines]\n",
        "    # print(word_pairs[200])\n",
        "\n",
        "  return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMIZvr_j5pR4"
      },
      "source": [
        "mono_de = openFile(path_mono_de)\n",
        "mono_en = openFile(path_mono_en)\n",
        "mono_zh = openFile(path_mono_zh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmQh34yLwMU"
      },
      "source": [
        "mono_de_test = openFile(path_mono_de_test)\n",
        "mono_en_test = openFile(path_mono_en_test)\n",
        "mono_zh_test = openFile(path_mono_zh_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxvnePwYBa_O"
      },
      "source": [
        "translate_en2de(u'i am my whole on my section.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swrznQWY_jFm"
      },
      "source": [
        "#BLEU score of Chinese-German Translation\n",
        "score = 0\n",
        "for i in range(100):\n",
        "  strinfo2 = re.compile('/')\n",
        "  sentence_cn2en = translate(mono_zh_test[i])\n",
        "  sentence_cn2en = strinfo2.sub('', sentence_cn2en).strip()\n",
        "  result = sentence_cn2en.split()\n",
        "  # print(result)\n",
        "  sen_val = preprocess_sentence(mono_de_test[i]) \n",
        "  # print(sen_val)\n",
        "  reference = [cleanSentence(sen_val).split()]\n",
        "  # print(reference)\n",
        "  score =score+ sentence_bleu(reference, result)\n",
        "print(score/100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}