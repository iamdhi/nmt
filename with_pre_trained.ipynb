{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "with pre trained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kV0og4T54cp",
        "outputId": "813ad522-8e96-40ab-842e-3a2899aa06f5"
      },
      "source": [
        "pip install opencc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (765 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 194 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 256 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 266 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 317 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 327 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 337 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 378 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 389 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 399 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 409 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 440 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 450 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 460 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 471 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 501 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 512 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 522 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 532 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 542 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 563 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 573 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 583 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 593 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 604 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 614 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 634 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 645 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 655 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 665 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 675 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 686 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 706 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 716 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 727 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 737 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 747 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 757 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 765 kB 13.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cZSy8qY6MvX",
        "outputId": "dd6c167c-5441-4eb9-8ea3-59c3176d93d3"
      },
      "source": [
        "pip install bpemb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 20.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.62.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VTbq9MK6Znm"
      },
      "source": [
        "from bpemb import BPEmb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj6k4VtU6Wmn",
        "outputId": "0f002224-85a9-4a75-f744-99222307228b"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\", dim=100,vs=100000)\n",
        "bpemb_zh = BPEmb(lang=\"zh\", dim=100,vs=100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1987533/1987533 [00:00<00:00, 28721361.08B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37969196/37969196 [00:00<00:00, 48894733.10B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1849493/1849493 [00:00<00:00, 25646205.10B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37914051/37914051 [00:00<00:00, 53349423.88B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu4GZUJt_vW-",
        "outputId": "a599c035-0ead-429b-8f98-5a489b76dc7f"
      },
      "source": [
        "bpemb_de = BPEmb(lang=\"de\", dim=100,vs=100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/de/de.wiki.bpe.vs100000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2068307/2068307 [00:00<00:00, 29602927.58B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/de/de.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 38035802/38035802 [00:00<00:00, 50142937.35B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v01ky_wd6hrt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import jieba\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from opencc import OpenCC\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHyrB9CTudFG"
      },
      "source": [
        "class Encoder_cn(tf.keras.Model):\n",
        "  def __init__(self, enc_units, batch_sz):\n",
        "    super(Encoder_cn, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    # self.embedding = tf.keras.layers.Embedding(100000, 100)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    print('shape1111:  ')\n",
        "    print('shape:  '+str(x.shape))\n",
        "    print('type:  '+str(type(x)))\n",
        "    print(x)\n",
        "    \n",
        "    # def printaa():\n",
        "    #   tf.print('aaaaaaaa')\n",
        "    # def my_numpy_func(a):\n",
        "    #   tf.print('aa  :'+str(type(a)))\n",
        "    #   a =  np.array(a)\n",
        "    #   tf.print('aaa  :'+str(type(a)))\n",
        "    #   # tf.print(s)\n",
        "    #   emb=bpemb_zh.embed(bpemb_zh.decode_ids(a[0]))\n",
        "    #   for i in range(1,64):\n",
        "    #     b = bpemb_zh.embed(bpemb_zh.decode_ids(a[i]))\n",
        "    #     emb= np.append(emb,b)\n",
        "    #   emb = emb.reshape(64,15,100)\n",
        "    #   return emb\n",
        "    # @tf.function(input_signature=[tf.TensorSpec((64,15), tf.int64)])\n",
        "    # def tf_function(input):\n",
        "    #   y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
        "    #   return y \n",
        "    # printaa()\n",
        "    # x= tf_function(x)\n",
        "\n",
        "    # print('shape:  '+str(x.shape))\n",
        "    # print('type:  '+str(type(x)))\n",
        "    # print(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ5UsD7yuii9"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # 隐藏层的形状 == （批大小，隐藏层大小）\n",
        "    # hidden_with_time_axis 的形状 == （批大小，1，隐藏层大小）\n",
        "    # 这样做是为了执行加法以计算分数  \n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # 分数的形状 == （批大小，最大长度，1）\n",
        "    # 我们在最后一个轴上得到 1， 因为我们把分数应用于 self.V\n",
        "    # 在应用 self.V 之前，张量的形状是（批大小，最大长度，单位）\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # 注意力权重 （attention_weights） 的形状 == （批大小，最大长度，1）\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # 上下文向量 （context_vector） 求和之后的形状 == （批大小，隐藏层大小）\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ6jf-8-ummm"
      },
      "source": [
        "class Decoder_en(tf.keras.Model):\n",
        "  def __init__(self, dec_units, batch_sz):\n",
        "    super(Decoder_en, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    # self.embedding = tf.keras.layers.Embedding(100000, 100)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(100000)\n",
        "\n",
        "    # 用于注意力\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # 编码器输出 （enc_output） 的形状 == （批大小，最大长度，隐藏层大小）\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    tf.print('context_vector  :'+str(context_vector))\n",
        "    tf.print('context_vector  :'+str(context_vector.shape))\n",
        "    # # x 在通过嵌入层后的形状 == （批大小，1，嵌入维度）\n",
        "\n",
        "    # def my_numpy_func(a):\n",
        "    #   a =  np.array(a)\n",
        "    #   # tf.print(s)\n",
        "    #   emb=bpemb_en.embed(bpemb_en.decode_ids(a[0]))\n",
        "    #   for i in range(1,64):\n",
        "    #     b = bpemb_en.embed(bpemb_en.decode_ids(a[i]))\n",
        "    #     emb= np.append(emb,b)\n",
        "    #   emb = emb.reshape(64,1,100)\n",
        "    #   return emb\n",
        "    # # @tf.function\n",
        "    # def tf_function(input):\n",
        "    #   y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
        "    #   return y \n",
        "    # x= tf_function(s)\n",
        "    \n",
        "\n",
        "\n",
        "    # x = self.embedding(x)\n",
        "    # x=emb\n",
        "    # x 在拼接 （concatenation） 后的形状 == （批大小，1，嵌入维度 + 隐藏层大小）\n",
        "    # x 在通过嵌入层后的形状 == （批大小，1，嵌入维度）\n",
        "    # x = self.embedding(x)\n",
        "    # tf.print('embedding   '+str(x))\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    # 将合并后的向量传送到 GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # 输出的形状 == （批大小 * 1，隐藏层大小）\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    tf.print('output:shape '+str(output))\n",
        "\n",
        "    # 输出的形状 == （批大小，vocab）\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtukeR9yB4PQ"
      },
      "source": [
        "# example_input_batch, example_target_batch = next(iter(dataset_cn2en))\n",
        "# example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDJk1nCsFEx"
      },
      "source": [
        "# sample_hidden = encoder_test.initialize_hidden_state()\n",
        "# sample_output, sample_hidden = encoder_test(example_input_batch, sample_hidden)\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "# print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8yGIsMB6k-"
      },
      "source": [
        "# type(example_input_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUC6mzIprDY"
      },
      "source": [
        "# decoder_test = Decoder_en( units, BATCH_SIZE)\n",
        "\n",
        "# sample_decoder_output, _, _ = decoder_test(emb_en_dec, sample_hidden, sample_output)\n",
        "\n",
        "# print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge6ECAKtuvRt"
      },
      "source": [
        "d_model = 128\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=6000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "662vASKZu4Ex"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAeQxOxsu0Wf"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.002, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQPMoZ7evBNz",
        "outputId": "b998e568-1ba2-42cf-fe93-eeea346994d7"
      },
      "source": [
        "path_to_file = \"drive/MyDrive/Colab Notebooks/cmn-clean.txt\"\n",
        "path_to_file_de = \"drive/MyDrive/Colab Notebooks/deu.txt\"\n",
        "path_to_singleCorpus = \"drive/MyDrive/Colab Notebooks/single_corpus_zh.txt\"\n",
        "path_to_newCorpus = \"drive/MyDrive/Colab Notebooks/new_corpus.txt\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "input_file = open(path_to_file,\"r\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzNoJav8vjmK"
      },
      "source": [
        "cc = OpenCC('t2s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6gf49X6Iykx"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "    w = w.lower().strip()\n",
        "\n",
        "    w = cc.convert(w)\n",
        " \n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "\n",
        "    # w = re.sub(r\"([?.!,:：？。，！])\", r\" \\1 \", w)\n",
        "    w = re.sub(r\"[^\\u4e00-\\u9fa5\\u0080-\\uFFFF_a-zA-Z0-9?.!,:：？。，！']+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "    w = '| ' + w + ' /'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLlaB-LBnlS"
      },
      "source": [
        "# preprocess_sentence(u\"你大約遅了三天。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jndDPE3gI1t-"
      },
      "source": [
        "def create_dataset(path):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines]\n",
        "    # print(word_pairs[200])\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BrB95O-I4Aa"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPz-k5VI8oI"
      },
      "source": [
        "# def tokenize(lang):\n",
        "#   lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "#   lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "#   tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "#   tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "#   print (tensor),print(lang_tokenizer)\n",
        "#   return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7MQfY9TI-Ky"
      },
      "source": [
        "def uniformLength_en_cn(list1,list2,maxlen,minlen):\n",
        "  print(len(list1),len(list2))\n",
        "  length = len(list1)\n",
        "  a= 0\n",
        "  l =0\n",
        "  while length != l:\n",
        "    l = len(list1)\n",
        "    for i in range(a,length):\n",
        "      if (len(bpemb_en.encode_ids(list1[i]))> maxlen or len(bpemb_en.encode_ids(list1[i]))< minlen\n",
        "          or len(bpemb_zh.encode_ids(list2[i]))> maxlen or len(bpemb_zh.encode_ids(list2[i]))< minlen):\n",
        "        del list1[i]\n",
        "        del list2[i]\n",
        "        length = len(list2)\n",
        "        a=i\n",
        "        break\n",
        "      else:\n",
        "        list1[i] = list1[i] + ' pad'*(maxlen-len(bpemb_en.encode_ids(list1[i])))\n",
        "        list2[i] = list2[i] + ' pad'*(maxlen-len(bpemb_zh.encode_ids(list2[i])))\n",
        "    \n",
        "  # print(len(list1),len(list2))\n",
        "  # print(list2[100])\n",
        "  return list2,list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Fl4uJX7_Fw"
      },
      "source": [
        "def cutDataset_en_cn(targ_lang,inp_lang,maxlen,minlen):\n",
        "    targ_lang=list(targ_lang)\n",
        "    inp_lang=list(inp_lang)\n",
        "    targ_lang, inp_lang =uniformLength_en_cn(targ_lang, inp_lang,maxlen,minlen)\n",
        "    return inp_lang, targ_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8duWzFsLEOwV"
      },
      "source": [
        "def uniformLength_en_de(list1,list2,maxlen,minlen):\n",
        "  print(len(list1),len(list2))\n",
        "  length = len(list1)\n",
        "  a= 0\n",
        "  l =0\n",
        "  while length != l:\n",
        "    l = len(list1)\n",
        "    for i in range(a,length):\n",
        "      if (len(bpemb_en.encode_ids(list1[i]))> maxlen or len(bpemb_en.encode_ids(list1[i]))< minlen\n",
        "          or len(bpemb_de.encode_ids(list2[i]))> maxlen or len(bpemb_de.encode_ids(list2[i]))< minlen):\n",
        "        del list1[i]\n",
        "        del list2[i]\n",
        "        length = len(list2)\n",
        "        a=i\n",
        "        break\n",
        "      else:\n",
        "        list1[i] = list1[i] + ' pad'*(maxlen-len(bpemb_en.encode_ids(list1[i])))\n",
        "        list2[i] = list2[i] + ' pad'*(maxlen-len(bpemb_de.encode_ids(list2[i])))\n",
        "\n",
        "      \n",
        "  # print(len(list1),len(list2))\n",
        "\n",
        "  return list2,list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUygm8a9EOwW"
      },
      "source": [
        "def cutDataset_en_de(targ_lang,inp_lang,maxlen,minlen):\n",
        "    targ_lang=list(targ_lang)\n",
        "    inp_lang=list(inp_lang)\n",
        "    targ_lang, inp_lang =uniformLength_en_de(targ_lang, inp_lang,maxlen,minlen)\n",
        "    return inp_lang, targ_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pr6eVld7obE",
        "outputId": "7386ba2f-d965-41ce-979a-bd5efdfe0d71"
      },
      "source": [
        "en_cn, cn = create_dataset(path_to_file)\n",
        "en_cn, cn =cutDataset_en_cn(en_cn, cn,15,1)\n",
        "# en_de, de = create_dataset(path_to_file_de)\n",
        "# en_de, de =cutDataset_en_de(en_de, de,15,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26828 26828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0aDz2bAlaRe"
      },
      "source": [
        "# cn[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Py0EoS_rFI"
      },
      "source": [
        "# bpemb_zh.embed(cn[3133]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a18MvBJf3tK"
      },
      "source": [
        "# cn_emb=bpemb_zh.embed(cn[0])\n",
        "# for i in range(1,len(cn)):\n",
        "# # for i in range(1,64):\n",
        "#   a = bpemb_zh.embed(cn[i])\n",
        "#   cn_emb= np.append(cn_emb,a)\n",
        "# cn_emb = cn_emb.reshape(len(cn),15,100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX1VKLw3dx4X"
      },
      "source": [
        "# en_cn_emb=bpemb_en.embed(en_cn[0])\n",
        "# for i in range(1,len(en_cn)):\n",
        "# # for i in range(1,64):\n",
        "#   a = bpemb_en.embed(en_cn[i])\n",
        "#   en_cn_emb= np.append(en_cn_emb,a)\n",
        "# en_cn_emb = en_cn_emb.reshape(len(en_cn),15,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuvLWNpSVAs"
      },
      "source": [
        "# cn_enc=bpemb_zh.encode_ids(cn[0])\n",
        "# for i in range(1,len(cn)):\n",
        "# # for i in range(1,10):\n",
        "#   a = bpemb_zh.encode_ids(cn[i])\n",
        "#   cn_enc= np.append(cn_enc,a)\n",
        "# cn_enc = cn_enc.reshape(len(cn),15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIkz8ZSDaLOW"
      },
      "source": [
        "# en_enc=bpemb_en.encode_ids(en_cn[0])\n",
        "# for i in range(1,len(en_cn)):\n",
        "# # for i in range(1,10):\n",
        "#   a = bpemb_en.encode_ids(en_cn[i])\n",
        "#   en_enc= np.append(en_enc,a)\n",
        "# en_enc = en_enc.reshape(len(en_cn),15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF5wxUJYWYPX"
      },
      "source": [
        "# type(cn_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_6JPm8FXUZO"
      },
      "source": [
        "# def convert_cn(tensor):\n",
        "#   for t in tensor:\n",
        "#     if t!=-1:\n",
        "#       print (\"%d ----> %s\" % (t, bpemb_zh.decode_ids([int(t)])))\n",
        "# def convert_en(tensor):\n",
        "#   for t in tensor:\n",
        "#     if t!=6643:\n",
        "#       print (\"%d ----> %s\" % (t, bpemb_en.decode_ids([int(t)])))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA0U3hweXmx5"
      },
      "source": [
        "# print (\"Input Language; index to word mapping\")\n",
        "# convert_cn(bpemb_zh.encode_ids(\"| 你大约 ⁇ 了三天。 / pad pad pad pad pad pad pad\"))\n",
        "# print ()\n",
        "# print (\"Target Language; index to word mapping\")\n",
        "# convert_en(en_cn_tensor_train[222])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EC0L8nSJOcu",
        "outputId": "316ed0bc-8546-473b-8dc7-ce4640b608cf"
      },
      "source": [
        "# 采用 80 - 20 的比例切分训练集和验证集\n",
        "en_cn_tensor_train, en_cn_tensor_val, cn_tensor_train, cn_tensor_val = train_test_split(en_cn, cn, test_size=0.1)\n",
        "\n",
        "# 显示长度\n",
        "print(len(en_cn_tensor_train), len(cn_tensor_train), len(en_cn_tensor_val), len(cn_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22995 22995 2555 2555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pef5UrsQfCaB"
      },
      "source": [
        "BUFFER_SIZE = len(cn_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(cn_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "# vocab_inp_size = len(inp_lang.word_index)+1\n",
        "# vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset_cn2en = tf.data.Dataset.from_tensor_slices((cn_tensor_train, en_cn_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset_cn2en = dataset_cn2en.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAOIKfWzupA4"
      },
      "source": [
        "encoder_bpe_cn = Encoder_cn(units, BATCH_SIZE)\n",
        "attention_layer_bpe_cn = BahdanauAttention(10)\n",
        "decoder_bpe_en = Decoder_en(units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIHwk88MuO3C"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_bpe_cn2en'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint_bpe_cn2en = tf.train.Checkpoint(optimizer=optimizer, encoder_bpe_cn=encoder_bpe_cn, decoder_bpe_en=decoder_bpe_en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anawjZEdoMva"
      },
      "source": [
        "# cn_emb[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIjXohsUo7Vu"
      },
      "source": [
        "# x_de_inp = tf.expand_dims(bpemb_en.encode_ids(\"|\") * BATCH_SIZE, 1)\n",
        "# emb_en_dec=bpemb_en.embed(bpemb_en.decode_ids(x_de_inp[0].numpy()))\n",
        "# for i in range(1,len(x_de_inp)):\n",
        "#   a = bpemb_en.embed(bpemb_en.decode_ids(x_de_inp[i].numpy()))\n",
        "#   emb_en_dec= np.append(emb_en_dec,a)\n",
        "# emb_en_dec = emb_en_dec.reshape(64,1,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FW8fbwmuoZs"
      },
      "source": [
        "# tf.expand_dims(bpemb_en.encode_ids(\"|\") * BATCH_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccOSZmaz-eqE"
      },
      "source": [
        "# def my_numpy_func_targ_0(s):\n",
        "#   # tf.print('aa  :'+str(type(s)))\n",
        "#   s =  np.array(s)\n",
        "#   # tf.print(s)\n",
        "#   emb=bpemb_en.embed(bpemb_en.decode_ids(s[0]))\n",
        "#   for i in range(1,64):\n",
        "#     a = bpemb_en.embed(bpemb_en.decode_ids(s[i]))\n",
        "#     emb= np.append(emb,a)\n",
        "#   emb = emb.reshape(64,1,100)\n",
        "#   return emb\n",
        "#  #(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "# @tf.function(input_signature=[tf.TensorSpec((64,1), tf.int32)])\n",
        "# def tf_function_targ_0(input):\n",
        "#   y = tf.numpy_function(my_numpy_func_targ_0, [input], tf.float32)\n",
        "#   return y \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnDAP4bD_CoY"
      },
      "source": [
        "# dec_inp = tf.expand_dims(bpemb_en.encode_ids(\"|\") * BATCH_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy5dyILE-lYP"
      },
      "source": [
        "# dec_inp_emb = tf_function_targ_0(dec_inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM7lUHP_QoGX"
      },
      "source": [
        "# for (batch, (inp, targ)) in enumerate(dataset_cn2en.take(1)):\n",
        "#   x=inp\n",
        "#   y=targ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSZZomWpUMq7"
      },
      "source": [
        "def my_numpy_func_targ(s):\n",
        "  # tf.print(s.shape)\n",
        "  # s =  np.array(s)\n",
        "  # tf.print(len(s))\n",
        "  emb=bpemb_en.embed(s[0].decode('utf-8'))\n",
        "  for i in range(1,64):\n",
        "    a = bpemb_en.embed(s[i].decode('utf-8'))\n",
        "    emb= np.append(emb,a)\n",
        "  emb = emb.reshape(64,15,100)\n",
        "  return emb\n",
        "@tf.function(input_signature=[tf.TensorSpec((64,), tf.string)])\n",
        "def tf_function_targ(input):\n",
        "  y = tf.numpy_function(my_numpy_func_targ, [input], tf.float32)\n",
        "  return y \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbgVHmS8yXUe"
      },
      "source": [
        "# @tf.function\n",
        "def train_step_bpe_zh_en(inp,targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    print('start')\n",
        "    targ = tf_function_targ(targ)\n",
        "    tf.print('targ'+str(targ))\n",
        "    tf.print(targ.shape)\n",
        "    enc_output, enc_hidden = encoder_bpe_cn(inp, enc_hidden)\n",
        "    print('end')\n",
        "    print(enc_output)\n",
        "    dec_hidden = enc_hidden\n",
        "    print('teaching1')\n",
        "    # dec_input = tf.expand_dims(bpemb_en.encode_ids(\"|\") * BATCH_SIZE, 1)\n",
        "    dec_input = targ[:,0]\n",
        "    print('teaching2')\n",
        "    # 教师强制 - 将目标词作为下一个输入\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # 将编码器输出 （enc_output） 传送至解码器\n",
        "      print('teaching3')\n",
        "      predictions, dec_hidden, _ = decoder_bpe_en(dec_input, dec_hidden, enc_output)\n",
        "      print('teaching4')\n",
        "      print('predictions:  '+str(predictions))\n",
        "      print('dec_hidden:  '+str(dec_hidden))\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      print('loss'+str(loss))\n",
        "\n",
        "      # 使用教师强制\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "      \n",
        "      # dec_input = tf_function_targ(y_en)\n",
        "      print('dec_input:  '+str(dec_input))\n",
        "      print('teaching6')\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder_bpe_cn.trainable_variables + decoder_bpe_en.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTzSjpbVuev"
      },
      "source": [
        "for (batch, (inp, targ)) in enumerate(dataset_cn2en.take(1)):\n",
        "  x=inp\n",
        "  y=targ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49l4lwRhZpIF"
      },
      "source": [
        "aaa=tf_function_targ(targ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM6vOIWIZx9U",
        "outputId": "1b4ba048-4ced-456c-aa07-65377797980b"
      },
      "source": [
        "aaa.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnImY7MTAwXa"
      },
      "source": [
        "def my_numpy_func(s):\n",
        "  # tf.print('aa  :'+str(type(s)))\n",
        "  # s =  np.array(s)\n",
        "  # tf.print('ssssssss')\n",
        "  # tf.print(s[0].decode('utf-8'))\n",
        "  emb=bpemb_zh.embed(s[0].decode('utf-8'))\n",
        "  for i in range(1,64):\n",
        "    a = bpemb_zh.embed(s[i].decode('utf-8'))\n",
        "    emb= np.append(emb,a)\n",
        "  emb = emb.reshape(64,15,100)\n",
        "  return emb\n",
        " #(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "@tf.function(input_signature=[tf.TensorSpec((64,), tf.string)])\n",
        "def tf_function(input):\n",
        "  y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
        "  return y \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7zfpYbRyXUh"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder_bpe_cn.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset_cn2en.take(steps_per_epoch)):\n",
        "    # def my_numpy_func(a):\n",
        "    #   tf.print('aa  :'+str(type(a)))\n",
        "    #   a =  np.array(a)\n",
        "    #   tf.print('aaa  :'+str(type(a)))\n",
        "    #   # tf.print(s)\n",
        "    #   emb=bpemb_zh.embed(bpemb_zh.decode_ids(a[0]))\n",
        "    #   for i in range(1,64):\n",
        "    #     b = bpemb_zh.embed(bpemb_zh.decode_ids(a[i]))\n",
        "    #     emb= np.append(emb,b)\n",
        "    #   emb = emb.reshape(64,15,100)\n",
        "    #   return emb\n",
        "    # @tf.function(input_signature=[tf.TensorSpec((64,15), tf.int64)])\n",
        "    # def tf_function(input):\n",
        "    #   y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
        "    #   return y \n",
        "    inp_x= tf_function(inp)\n",
        "    # print(x.shape)\n",
        "\n",
        "    batch_loss = train_step_bpe_zh_en(inp_x,targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # 每 2 个周期（epoch），保存（检查点）一次模型\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint_bpe_cn2en.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}